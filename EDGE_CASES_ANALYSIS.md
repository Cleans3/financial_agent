# Financial Agent - Edge Cases Analysis ğŸ“‹

**Document Version**: 1.0  
**Last Updated**: December 24, 2025  
**Project**: Vietnamese Stock Market Investment Assistant (Financial Agent)

---

## ğŸ“– Table of Contents

1. [Use Cases](#use-cases)
2. [Architecture Overview](#architecture-overview)
3. [Workflow Pipeline](#workflow-pipeline)
4. [Critical Edge Cases](#critical-edge-cases)
5. [Data Flow Edge Cases](#data-flow-edge-cases)
6. [File Handling Edge Cases](#file-handling-edge-cases)
7. [RAG & Retrieval Edge Cases](#rag--retrieval-edge-cases)
8. [Tool Execution Edge Cases](#tool-execution-edge-cases)
9. [State Management Edge Cases](#state-management-edge-cases)
10. [Error Handling Edge Cases](#error-handling-edge-cases)
11. [Performance & Scalability Edge Cases](#performance--scalability-edge-cases)
12. [Recommendation Matrix](#recommendation-matrix)

---

## Use Cases

### ğŸ‘¤ **Primary Users & Their Goals**

#### 1. **Individual Retail Investor** ğŸ“ˆ
**Profile**: Individual traders, small portfolio holders
- **Goals**: 
  - Get real-time stock prices and technical analysis
  - Understand financial ratios and company fundamentals
  - Make quick trading decisions
  - Track historical price trends

**Key Interactions**:
```
1. Opens Frontend
2. Types: "What's TCB's current price and 20-day moving average?"
3. System:
   â”œâ”€ Extracts ticker: "TCB"
   â”œâ”€ Calls tools: get_current_price(), calculate_sma()
   â”œâ”€ Returns: Price $23.50, SMA-20 $23.25
   â””â”€ Agent synthesizes: "TCB is trading above its 20-day average..."
4. User makes investment decision
```

**Success Criteria**:
- âœ… Response within 3 seconds
- âœ… Accurate price data (< 5 min old)
- âœ… Correct technical calculations
- âœ… Clear explanations

**Common Queries**:
- "Which banks have highest P/E ratio?"
- "Show me VCB's historical revenue"
- "Compare TCB vs VCB dividend yields"
- "Is FPT overvalued based on P/E?"

---

#### 2. **Financial Analyst** ğŸ“Š
**Profile**: Professional analysts, fund managers, financial advisors
- **Goals**:
  - Analyze large datasets (quarterly reports, 5-year financials)
  - Compare multiple companies systematically
  - Extract insights from unstructured documents
  - Generate investment reports

**Key Interactions**:
```
1. Uploads Q3 2024 Financial Reports (5 PDF files)
2. Types: "Summarize revenue growth trends across all companies"
3. System:
   â”œâ”€ [EXTRACT_DATA] Processes all PDFs with OCR
   â”œâ”€ [INGEST_FILE] Stores chunks in personal RAG
   â”œâ”€ [AGENT] Searches RAG for revenue data
   â”œâ”€ Aggregates: "FPT +15%, VCB +8%, TCB +5%..."
   â””â”€ Generates markdown report with tables
4. Analyst downloads report for client presentation
```

**Success Criteria**:
- âœ… Process 50MB PDF in < 1 minute
- âœ… Maintain document privacy (no mix with other users)
- âœ… Accurate data extraction (> 95% accuracy)
- âœ… Structured output (markdown tables, charts)

**Common Queries**:
- "What's the average profit margin across uploaded companies?"
- "Which company shows strongest growth in assets?"
- "Extract debt-to-equity ratios from all reports"
- "Create comparison table of top 5 metrics"

---

#### 3. **Risk Manager** ğŸ›¡ï¸
**Profile**: Bank risk officers, fund risk managers
- **Goals**:
  - Monitor market volatility indicators (RSI, MACD)
  - Identify risk trends and anomalies
  - Set alerts for threshold breaches
  - Generate compliance reports

**Key Interactions**:
```
1. Asks: "Show RSI for top 20 stocks, flag any > 70 (overbought)"
2. System:
   â”œâ”€ Iterates through ticker list
   â”œâ”€ Calculates RSI for each
   â”œâ”€ Filters: RSI > 70
   â”œâ”€ Returns: 5 stocks above threshold
   â””â”€ Risk manager sets watch alerts
3. Monitors portfolio risk exposure
```

**Success Criteria**:
- âœ… Batch analysis of 20+ stocks < 5 seconds
- âœ… Accurate RSI calculations matching Bloomberg
- âœ… Real-time data (not > 5 min old)
- âœ… Consistent methodology across all tickers

**Common Queries**:
- "Which stocks are in overbought territory (RSI > 70)?"
- "Calculate correlation between TCB and VCB prices"
- "Show volatility metrics for last 90 days"
- "Alert if any stock drops > 10% from 50-day MA"

---

#### 4. **Student/Educator** ğŸ“
**Profile**: Finance students, investment course instructors
- **Goals**:
  - Learn stock market concepts
  - Understand technical indicators
  - Practice analysis on real data
  - Create educational materials

**Key Interactions**:
```
1. Student asks: "Explain the difference between SMA-20 and SMA-50"
2. System:
   â”œâ”€ Provides definition and purpose
   â”œâ”€ Calculates both for example stock (e.g., FPT)
   â”œâ”€ Shows chart/data comparison
   â”œâ”€ Explains trend signal: "Bullish if SMA-20 > SMA-50"
3. Student learns by example
```

**Success Criteria**:
- âœ… Clear, educational explanations
- âœ… Visual data representation
- âœ… Real-world examples with actual stock data
- âœ… No jargon without explanation

**Common Queries**:
- "What is P/E ratio and how to interpret it?"
- "Show me how to calculate Fibonacci retracement"
- "Explain RSI overbought/oversold conditions"
- "Compare growth stock vs value stock characteristics"

---

### ğŸ¯ **Use Case Workflows**

#### **UC1: Quick Price Check** âš¡
```
Input: User types "TCB price"
â”œâ”€ Classification: Financial query
â”œâ”€ Tool Selection: get_current_price()
â”œâ”€ Execution: VnStock API call
â”œâ”€ Response: "TCB is trading at $23.50"
Duration: < 1 second
Complexity: Simple
Tool Chain: 1 tool
```

---

#### **UC2: Technical Analysis Report** ğŸ“‰
```
Input: User types "Analyze FPT stock technical setup"
â”œâ”€ Classification: Technical analysis
â”œâ”€ Tool Selection: [
â”‚    - get_historical_data(FPT, last 3 months)
â”‚    - calculate_sma(FPT, windows=[20, 50, 200])
â”‚    - calculate_rsi(FPT, period=14)
â”‚  ]
â”œâ”€ Execution: 3 sequential tool calls
â”œâ”€ Synthesis: LLM merges results â†’ analysis
â”œâ”€ Response: "FPT shows bullish setup: price above SMA-20..."
Duration: 3-5 seconds
Complexity: Medium
Tool Chain: 3 tools
```

---

#### **UC3: Multi-Document Financial Analysis** ğŸ“‘
```
Input: User uploads 10 company Q3 reports (PDF)
       Query: "Which company has best profitability?"
       
Flow:
â”œâ”€ [EXTRACT_DATA] 
â”‚  â”œâ”€ Process 10 PDFs
â”‚  â”œâ”€ OCR scanned pages
â”‚  â”œâ”€ Extract text chunks
â”‚  â””â”€ Create embeddings
â”‚
â”œâ”€ [INGEST_FILE]
â”‚  â”œâ”€ Store chunks in personal RAG
â”‚  â”œâ”€ Index by company/section
â”‚  â””â”€ Record file metadata
â”‚
â”œâ”€ [AGENT]
â”‚  â”œâ”€ Search RAG: "profitability metrics"
â”‚  â”œâ”€ Retrieve net profit margins
â”‚  â”œâ”€ LLM extracts and compares
â”‚  â””â”€ Ranks companies by profitability
â”‚
â””â”€ Response: "Company A: 25% margin, Company B: 22%, ..."

Duration: 60-120 seconds (mostly file processing)
Complexity: High
Tool Chain: RAG search + LLM synthesis
File Types: PDF (with OCR)
```

---

#### **UC4: Comparative Analysis** ğŸ”„
```
Input: User asks "TCB vs VCB: which is cheaper by P/E?"

Flow:
â”œâ”€ Extract tickers: TCB, VCB
â”œâ”€ Tool calls (parallel):
â”‚  â”œâ”€ get_company_info(TCB) â†’ earnings
â”‚  â”œâ”€ get_current_price(TCB) â†’ price
â”‚  â”œâ”€ get_company_info(VCB) â†’ earnings
â”‚  â””â”€ get_current_price(VCB) â†’ price
â”œâ”€ LLM calculates P/E for both
â”œâ”€ Compares and ranks
â””â”€ Response: "VCB cheaper: P/E 8.5 vs TCB 10.2"

Duration: 3-5 seconds
Complexity: Medium
Tool Chain: 4 parallel tools
```

---

#### **UC5: Batch Analysis with Filtering** ğŸ“Š
```
Input: "Show all stocks with RSI > 70 (overbought) and price > 100K"

Flow:
â”œâ”€ Tool: get_all_stocks() â†’ [100+ tickers]
â”œâ”€ For each ticker:
â”‚  â”œâ”€ get_current_price()
â”‚  â”œâ”€ calculate_rsi()
â”‚  â””â”€ Filter by conditions
â”œâ”€ Aggregate results
â””â”€ Response: Table of 8 stocks meeting criteria

Duration: 10-20 seconds
Complexity: High (batch processing)
Tool Chain: Sequential iteration + filtering
Data: 100+ stocks processed
```

---

#### **UC6: Long Conversation with Context** ğŸ’¬
```
Turn 1:
â”œâ”€ User: "What's FPT's revenue?"
â”œâ”€ Agent: Calls tool, returns $X billion
â””â”€ Store message in history

Turn 2:
â”œâ”€ User: "How does that compare to last year?"
â”œâ”€ Agent: Needs FPT context from Turn 1
â”œâ”€ Calls get_historical_data(FPT)
â”œâ”€ Compares current vs previous
â””â”€ Response: "Up 15% YoY"

Turn 3:
â”œâ”€ User: "Is that growth rate sustainable?"
â”œâ”€ Agent: Uses FPT context from Turn 1 & 2
â”œâ”€ Reads financial fundamentals
â”œâ”€ Assesses sustainability
â””â”€ Response: Analysis based on metrics

Duration: 15 seconds (3 turns)
Complexity: High (multi-turn context)
Tool Chain: 2 tools across conversation
Context: Maintained across 3 turns
```

---

### ğŸ“‹ **Edge Cases Within Use Cases**

#### **UC2 Extended: What if RSI calculation fails?**
```
User: "Analyze FPT stock"

Normal flow:
â”œâ”€ get_historical_data(FPT) â†’ âœ“ 1000 data points
â”œâ”€ calculate_sma(FPT) â†’ âœ“ Returns moving averages
â”œâ”€ calculate_rsi(FPT) â†’ âœ— ERROR: "Need 14+ data points"

Edge case:
- Newly listed stock with only 5 trading days
- RSI requires 14+ periods
- Tool fails
- Agent sees error, continues with SMA only
- Answer: "FPT too new for RSI, but SMA shows..."
- User: Partially satisfied (missing one indicator)
```

---

#### **UC3 Extended: What if document is corrupted?**
```
User: Uploads 10 files: 9 PDFs + 1 corrupted ZIP

Processing:
â”œâ”€ File 1-8: âœ“ Extract successful
â”œâ”€ File 9: âœ— ZIP file â†’ Unsupported format â†’ Skip
â”œâ”€ File 10: âœ“ Extract successful
â”œâ”€ Result: 9/10 files ingested
â””â”€ User: Unaware that 1 file was skipped!

Risk: Incomplete analysis without user notification
```

---

#### **UC5 Extended: What if batch processing hangs?**
```
User: "Show all stocks with RSI > 70"
      (Requests analysis of 100+ stocks)

Scenario:
â”œâ”€ Process 10 stocks: 2 seconds âœ“
â”œâ”€ Process 20 stocks: 4 seconds âœ“
â”œâ”€ Process 50 stocks: 10 seconds âœ“
â”œâ”€ Process 80 stocks: 30 seconds âš ï¸
â”œâ”€ Process 100+ stocks: > 60 seconds âŒ
â”œâ”€ User gives up, closes browser
â”œâ”€ Backend keeps processing
â””â”€ Resources wasted

Risk: No UI feedback, user doesn't know if it's working
```

---

#### **UC6 Extended: What if context grows too large?**
```
Turn 1-10: Normal conversation, tokens OK
Turn 11-30: Still fine, ~50K tokens
Turn 31-50: Getting large, ~100K tokens
Turn 51: User asks new question
â”œâ”€ System tries to send all 50 turns to LLM
â”œâ”€ Total: 125K tokens > LLM limit (128K)
â”œâ”€ Error: 413 Payload Too Large
â”œâ”€ Chat breaks: User must start new conversation
â””â”€ Conversation history lost

Risk: Data loss after long conversation
```

---

### âœ… **Success Metrics by Use Case**

| Use Case | Metric | Target | Current Risk |
|----------|--------|--------|--------------|
| **UC1: Quick Price** | Latency | < 1s | âœ“ Likely met |
| | Accuracy | 100% | âœ“ API accurate |
| **UC2: Tech Analysis** | Latency | < 5s | ğŸŸ¡ Borderline |
| | Tool accuracy | 99%+ | ğŸŸ¡ RSI edge cases |
| **UC3: Multi-Doc** | File handling | 0 corruption | ğŸ”´ No validation |
| | Privacy | User-isolated | ğŸ”´ Default user_id |
| **UC4: Comparison** | Tool chaining | Works seamlessly | ğŸŸ¡ No error recovery |
| **UC5: Batch** | Timeout | < 30s | ğŸ”´ No timeout set |
| | Feedback | Real-time progress | âŒ Silent processing |
| **UC6: Conversation** | Context limit | No crash | ğŸ”´ > 128K crashes |
| | History preservation | All 50+ turns | ğŸŸ¡ Token overflow |

---

### ğŸ¨ **User Journey Maps**

#### **Analyst's Day**
```
9:00 AM
â”œâ”€ Morning: Check market overview
â”‚  â””â”€ "Which sectors are up today?"
â”‚
10:00 AM
â”œâ”€ Client request: Analyze 3 companies
â”‚  â”œâ”€ Uploads 3 Q3 reports
â”‚  â”œâ”€ System ingests and indexes
â”‚  â””â”€ Analyst prepares comparison
â”‚
11:00 AM
â”œâ”€ Client call: Present analysis
â”‚  â”œâ”€ Queries agent for supporting data
â”‚  â”œâ”€ "Show revenue breakdown by segment"
â”‚  â””â”€ Gets instant RAG results
â”‚
2:00 PM
â”œâ”€ Deep dive: Technical analysis
â”‚  â”œâ”€ "Calculate correlation matrix for top 10 stocks"
â”‚  â”œâ”€ Batch processing
â”‚  â””â”€ Generates report
â”‚
4:00 PM
â”œâ”€ Risk review: Set alerts
â”‚  â”œâ”€ "Alert if RSI > 70 for any watched stock"
â”‚  â””â”€ Monitoring begins
â”‚
5:00 PM
â””â”€ End of day: Export findings
   â””â”€ "Create markdown report of today's analysis"
```

**Pain Points**:
- âŒ File upload can't be cancelled (UC3 edge case)
- âŒ Batch queries have no progress bar (UC5 edge case)
- âŒ Long conversations eventually crash (UC6 edge case)

---

### ğŸš¨ **High-Risk Use Cases**

#### **Risk UC1: Regulatory Compliance Report**
```
Scenario: Risk manager must generate audit report
Input: "Extract all risk metrics from uploaded documents"

Requirements:
- âœ… Data accuracy: 100% (regulatory requirement)
- âœ… Data source tracking (where each number came from)
- âœ… Timestamp of data (when was it fetched)
- âœ… Audit trail (who accessed what)

Current System:
- âŒ No source tracking
- âŒ No timestamp on tool results
- âŒ No audit logging
- âŒ Default user_id = potential data leak

Risk Impact: **Regulatory failure, fines**
```

---

#### **Risk UC2: Critical Trade Execution**
```
Scenario: Trader relies on agent for price check before trade
Input: "What's FPT's current price? I'm about to execute 10M share trade"

Requirements:
- âœ… Data freshness: < 1 minute old
- âœ… Data accuracy: Â±0% error acceptable
- âœ… Responsiveness: < 2 seconds

Current System:
- âš ï¸  API could return cached data 5+ minutes old
- âš ï¸  No freshness indicator in response
- âœ“ Response time likely OK

Risk Impact: **Trader executes at wrong price, loses money**
```

---

#### **Risk UC3: Portfolio Manager Multi-User Collision**
```
Scenario: 3 portfolio managers using system simultaneously
Manager A: Uploads portfolio files
Manager B: Uploads portfolio files
Manager C: Uploads portfolio files

All use system default user_id="default"

Risk:
- Manager A's portfolio visible to B & C
- Wrong investment decisions
- Data breach, fiduciary violation

Current System:
- âŒ Defaults to "default" if user_id missing
- âŒ No validation of user_id
- âŒ No multi-tenancy isolation tests

Risk Impact: **Legal liability, license revocation**
```

---

## âœ¨ **Recommended Enhancements**

### For Each Use Case:

| Use Case | Enhancement | Effort | Impact |
|----------|-------------|--------|--------|
| **UC1** | Add "Data as of" timestamp | Low | High |
| **UC2** | Cache intermediate tool results | Medium | Medium |
| **UC3** | Batch file validation | Low | High |
| **UC4** | Parallel tool execution | Medium | High |
| **UC5** | Progress bar + cancellation | Medium | High |
| **UC6** | Token counting + auto-summarization | High | High |

---

## Architecture Overview

The Financial Agent is a multi-layered system with the following components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Server (main.py)                     â”‚
â”‚              (Port 8000, CORS + Security Headers)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Upload  â”‚      â”‚   Chat   â”‚      â”‚Admin API â”‚
   â”‚Endpoint â”‚      â”‚ Endpoint â”‚      â”‚Endpoint  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   LangGraph Workflow Engine     â”‚
        â”‚  (4-Node: Extractâ†’Ingestâ†’     â”‚
        â”‚   Agentâ†’Tools)                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                              â”‚
        â–¼                              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚File Process â”‚          â”‚ RAG Service      â”‚
   â”‚Pipeline     â”‚          â”‚ - Personal RAG   â”‚
   â”‚- PDF,Excel  â”‚          â”‚ - Global RAG     â”‚
   â”‚- OCR        â”‚          â”‚ - Semantic Searchâ”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚ - Keyword Search â”‚
          â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Qdrant Vector â”‚
   â”‚Database      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

| Component | Purpose | Key Features |
|-----------|---------|--------------|
| **FastAPI App** | REST API Server | Auth, Rate Limiting, CORS |
| **LangGraph Workflow** | Orchestration Engine | 4-node pipeline with conditional routing |
| **FinancialAgent** | Tool & LLM Manager | 8+ financial tools, LLM integration |
| **File Pipeline** | File Processing | PDF, Excel, Image (OCR) support |
| **RAG Service** | Semantic Search | Multi-collection, user-isolated storage |
| **Qdrant Vector DB** | Vector Storage | Persistent embeddings, metadata filtering |

---

## Workflow Pipeline

### High-Level Flow

```
USER INPUT
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT_DATA NODE     â”‚  â† File uploaded?
â”‚  - Process files (PDF,  â”‚
â”‚    Excel, Images)       â”‚
â”‚  - Extract text chunks  â”‚
â”‚  - Create structured    â”‚
â”‚    data                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INGEST_FILE NODE      â”‚  â† Chunks extracted?
â”‚  - Embed chunks         â”‚
â”‚  - Store in personal    â”‚
â”‚    RAG (user isolated)  â”‚
â”‚  - Record file IDs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AGENT NODE (Pass 1)   â”‚  â† Initial decision
â”‚  - Read user query      â”‚
â”‚  - Prepare RAG context  â”‚
â”‚  - Invoke LLM with toolsâ”‚
â”‚  - Detect tool calls    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                   â”‚
     â”‚ Tools Called?     â”‚
     â”œâ”€ YES: â–¼           â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
     â”‚   â”‚ TOOLS NODE  â”‚ â”‚
     â”‚   â”‚ - Execute   â”‚ â”‚
     â”‚   â”‚ - Get results
     â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
     â”‚          â”‚        â”‚
     â”‚          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚               â”‚
     â”‚ NO: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€â–¶ Continue to
         AGENT NODE
         (Pass 2)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ AGENT NODE      â”‚
    â”‚ (Final Synthesis)
    â”‚ - Merge RAG +   â”‚
    â”‚   Tool Results  â”‚
    â”‚ - Generate      â”‚
    â”‚   Final Answer  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
         RETURN
       FINAL ANSWER
```

### State Evolution

```
Initial State
â”œâ”€â”€ user_prompt: "What is TCB's SMA-20?"
â”œâ”€â”€ uploaded_files: []
â”œâ”€â”€ conversation_history: []
â”œâ”€â”€ user_id: "user123"
â””â”€â”€ session_id: "session456"
    â”‚
    â”œâ”€ [EXTRACT_DATA] â”€â–¶ extracted_file_data: null
    â”‚
    â”œâ”€ [INGEST_FILE] â”€â”€â–¶ ingested_file_ids: []
    â”‚
    â”œâ”€ [AGENT PASS 1] â”€â–¶ conversation_history: [
    â”‚                      HumanMessage(...),
    â”‚                      AIMessage(tool_calls=[...])
    â”‚                    ]
    â”‚
    â”œâ”€ [TOOLS] â”€â”€â”€â”€â”€â”€â”€â”€â–¶ conversation_history: [...,
    â”‚                      ToolMessage(result=...)
    â”‚                    ]
    â”‚
    â””â”€ [AGENT PASS 2] â”€â–¶ conversation_history: [...,
                           AIMessage(content="Final answer...")
                         ]
                         generated_answer: "Final answer..."
```

---

## Critical Edge Cases

### ğŸ”´ 1. NULL/EMPTY STATE PROPAGATION

**Scenario**: What happens when key fields are None or empty?

| Field | Edge Case | Current Behavior | Risk | Severity |
|-------|-----------|------------------|------|----------|
| `user_prompt` | Empty string `""` | Agent proceeds with empty query | LLM gets no context, generates generic response | ğŸŸ¡ MEDIUM |
| `uploaded_files` | `None` vs `[]` | Extract node skips both | State inconsistency | ğŸŸ¡ MEDIUM |
| `conversation_history` | Empty `[]` | Agent creates first message | First message has no context | ğŸŸ¢ LOW |
| `best_search_results` | `None` | RAG context not applied | Answers miss document insights | ğŸ”´ HIGH |
| `user_id` | "default" (fallback) | Multi-user collisions possible | Data isolation breach | ğŸ”´ HIGH |
| `session_id` | "default" (fallback) | Multiple sessions mixed in RAG | Conversation leakage | ğŸ”´ HIGH |

**Recommended Fixes**:
```python
# âŒ Current: allows None propagation
user_id = state.get("user_id", "default")

# âœ… Better: validate required fields
user_id = state.get("user_id")
if not user_id:
    raise ValueError("user_id is required for RAG isolation")

# âœ… Best: use proper defaults with validation
user_id = state.get("user_id") or uuid4()  # Generate unique ID if missing
session_id = state.get("session_id") or uuid4()
```

---

### ğŸ”´ 2. FILE SIZE & TYPE VALIDATION BYPASS

**Scenario**: What if someone uploads a 10GB file or unsupported format?

**Current State**:
- Document Service has 50MB limit âœ…
- Supported formats: PDF, DOCX, TXT, PNG, JPG âœ…
- BUT: No validation at API upload endpoint âŒ

**Edge Cases**:
1. **Zero-byte file**: `len(file_data) == 0`
   - Pipeline extracts no chunks
   - Ingest node processes empty list
   - User thinks file was processed
   - **Risk**: Silent failure

2. **Corrupted PDF**: File has `.pdf` extension but binary corruption
   - pdfplumber fails silently or throws exception
   - Pipeline catches exception
   - Returns `"success": false` in extracted_data
   - **Risk**: Partial ingestion (some chunks succeed, some fail)

3. **Image with no text**: Picture of a graph (no OCR text)
   - Tesseract returns empty string
   - Creates 0-content chunks
   - Wastes database space
   - **Risk**: Bloated vector store, poor search results

4. **Mixed file types in batch upload**:
   - 5 PDFs + 1 Excel + 1 corrupted ZIP
   - Pipeline processes sequentially
   - Stops at ZIP (unsupported)
   - Earlier files already ingested
   - Later files never processed
   - **Risk**: Incomplete ingestion with no user notification

**Example Code**:
```python
# From: src/core/langgraph_workflow.py, node_extract_data
for file_info in uploaded_files:
    try:
        result = pipeline.process(file_path, file_type, file_name)
        extracted_data[file_name] = {
            "success": True,
            "content": result.get("text", ""),  # âš ï¸  Could be empty!
            "chunks": result.get("chunks", [])   # âš ï¸  Could be []!
        }
    except Exception as e:
        # Silently logs error and continues
        extracted_data[file_name] = {"success": False, "error": str(e)}
```

---

### ğŸ”´ 3. TOOL EXECUTION CASCADING FAILURES

**Scenario**: What if a tool crashes during execution?

**Current Implementation**:
```python
# From node_tools in langgraph_workflow.py
for tool in tool_calls:
    try:
        # Execute tool
        result = tool_executor.invoke(...)
        messages.append(ToolMessage(content=result))
    except Exception as e:
        # Returns error message
        error_message = AIMessage(content=f"Tool error: {str(e)}")
        messages.append(error_message)
```

**Edge Cases**:

| Scenario | Tool Example | Current Behavior | Issue |
|----------|--------------|------------------|-------|
| **Tool timeout** | get_historical_data (1000 days) | Hangs indefinitely | Request never completes |
| **API rate limit** | VnStock API limit exceeded | Exception caught, error message | Agent doesn't retry/backoff |
| **Invalid parameters** | SMA with window > data points | Tool throws ValueError | Error message in history |
| **Network error** | VnStock API unreachable | Socket timeout | Treated as execution error |
| **Partial results** | get_company_info returns None for some fields | Processing continues | Null fields propagate to answer |
| **Tool calls themselves** | Tool A calls Tool B (chain) | No mechanism for chaining | Must return result in single call |

**Risk Scenarios**:
1. **Infinite retry loop**: Agent sees error, decides to call tool again â†’ same error â†’ loop
2. **State corruption**: Tool partially modifies database, crashes, rolls back silently
3. **Resource exhaustion**: Multiple concurrent tool calls exhaust connection pool
4. **Answer quality**: Graceful error message looks like valid answer to user

---

### ğŸ”´ 4. RAG CONTEXT MISMATCH WITH TOOL RESULTS

**Scenario**: RAG returns one answer, tool returns different answer

**Example**:
- **RAG**: "TCB's latest revenue is $500M (from 2024 Q3 report)"
- **Tool**: "TCB's latest stock price is $25 (from VnStock API)"
- **Conflict**: Different data sources, different freshness
- **Agent must decide**: Which to trust?

**Current Behavior**:
```python
# From node_agent in langgraph_workflow.py
if rag_context:
    system_text += "\nğŸ“š TÃ i liá»‡u liÃªn quan:\n"
    for i, doc in enumerate(rag_context[:5], 1):
        system_text += f"  {i}. {title} (score: {score:.1%})\n"

# Then later:
response = chain.invoke({"messages": messages})  # LLM merges both!
```

**Problems**:
1. **No conflict resolution**: LLM decides based on prompt, not data freshness
2. **Hallucination risk**: LLM might fabricate reconciliation
3. **User confusion**: Answer doesn't cite which source was primary
4. **Potential contradictions**: "Report says revenue is $X, but stock trades at $Y (contradiction!)"

---

### ğŸ”´ 5. CIRCULAR TOOL DEPENDENCIES

**Scenario**: Tool A needs output of Tool B, but Tool B needs output of Tool A

**Example Dependency Chain**:
```
User: "Calculate RSI for TCB over last 30 days"
     â”‚
     â”œâ”€ Tool: calculate_rsi()
     â”‚  â””â”€ Needs: historical_prices
     â”‚     â”‚
     â”‚     â””â”€ Tool: get_historical_data()
     â”‚        â””â”€ Needs: ticker ("TCB")
     â”‚           âœ“ From user prompt
     â”‚
     â””â”€ Success: RSI calculated
```

**Problem Scenario** (Hypothetical):
```
User: "Calculate SMA for the stock mentioned in the document"
     â”‚
     â”œâ”€ Tool: calculate_sma()
     â”‚  â””â”€ Needs: ticker
     â”‚     â”‚
     â”‚     â””â”€ Extract from doc (but which doc? multiple uploaded)
     â”‚        â”‚
     â”‚        â””â”€ Tool: search_documents()  
     â”‚           â””â”€ Needs: search_query
     â”‚              â”‚
     â”‚              â””â”€ Generated from user prompt...
     â”‚                 â”œâ”€ "the stock" (ambiguous!)
     â”‚                 â”œâ”€ Interpreted as FIRST ticker found
     â”‚                 â””â”€ If wrong ticker, SMA is wrong
     â”‚
     â””â”€ Silent failure: SMA calculated for wrong ticker!
```

**Current Implementation Has No**:
- âœ— Dependency resolution
- âœ— Multi-step tool chains
- âœ— Parameter validation between tools
- âœ— Circularity detection

---

## Data Flow Edge Cases

### ğŸŸ¡ 6. ENCODING & UNICODE ISSUES

**Scenario**: Vietnamese text with special characters

**Edge Cases**:

| Case | Input | Risk | Example |
|------|-------|------|---------|
| **Mixed encodings** | PDF with UTF-8 + ISO-8859-1 | Mojibake | "Tiáº¿ng Viá»‡t" â†’ "TiÃ•â€°ng ViÃ•â€¡t" |
| **Emoji in prompt** | "CÃ´ng ty ğŸ“ˆ lÃ  gÃ¬?" | Tokenization breaks | LLM truncates after emoji |
| **RTL text** | Arabic/Hebrew in documents | Display issues | Search still works, UI breaks |
| **Tone marks** | "tÃ i chÃ­nh" vs "tÃ i chÃ­nh" (different marks) | Search misses | Similar meaning, exact match fails |
| **Control characters** | PDF with embedded null bytes | Parser crashes | `\x00` breaks string operations |

**Example Code**:
```python
# From document_service.py
def extract_text_from_image(self, image_path):
    # Uses Tesseract with default config
    text = pytesseract.image_to_string(image_path)  # âš ï¸  No encoding specified
    
    # If image has mixed encodings:
    # - May succeed but produce garbage
    # - May fail silently
    # - May throw exception with partial results
    return text
```

**Recommended Fix**:
```python
# âœ… Explicit encoding handling
import unicodedata

def normalize_text(text: str) -> str:
    """Normalize Vietnamese text"""
    # NFD decomposition for consistent handling of tone marks
    text = unicodedata.normalize('NFD', text)
    # Remove null bytes that break vector operations
    text = text.replace('\x00', '')
    return text
```

---

### ğŸŸ¡ 7. LARGE CONTEXT WINDOW OVERFLOW

**Scenario**: Many documents + long conversation history = exceeds LLM token limit

**Example**:
```
Conversation History:
â”œâ”€â”€ User: Query 1 (500 tokens)
â”œâ”€â”€ Agent: Response 1 (2000 tokens)
â”œâ”€â”€ User: Query 2 (500 tokens)
â”œâ”€â”€ Agent: Response 2 (2000 tokens)
â”œâ”€â”€ [... 50 turns later ...]
â””â”€â”€ User: Query 50 (500 tokens)
    Total: ~125,000 tokens

+ RAG Documents (5 results Ã— 2000 tokens each) = 10,000 tokens

+ System Prompt = 3,000 tokens

= 138,000 tokens > 128K Claude token limit âŒ
```

**Current Behavior**:
```python
# From node_agent
prompt = ChatPromptTemplate.from_messages([
    ("system", system_text),  # 3K tokens
    MessagesPlaceholder(variable_name="messages"),  # ALL history!
])
response = chain.invoke({"messages": messages})  # Sends everything to LLM
```

**Problems**:
1. **No token counting**: Sends full history without checking
2. **No truncation**: Oldest messages never pruned
3. **No summarization**: Can't compress history
4. **Hard failure**: LLM returns 413 Payload Too Large
5. **User impact**: Chat becomes unusable after 50+ turns

---

### ğŸŸ¡ 8. RACE CONDITIONS IN CONCURRENT REQUESTS

**Scenario**: Two users upload files simultaneously

**Setup**:
```
User A (session_a): Uploads file_a.pdf
User B (session_b): Uploads file_b.pdf
            Both trigger workflow simultaneously
            Both use default user_id="default"  â† RACE CONDITION!
```

**Timeline**:
```
T0: User A â†’ Upload file_a.pdf
    T0+100ms: User B â†’ Upload file_b.pdf

T0+200ms: Workflow A starts â†’ [EXTRACT_DATA] for file_a
T0+250ms: Workflow B starts â†’ [EXTRACT_DATA] for file_b

T0+400ms: Workflow A â†’ [INGEST_FILE] for file_a.pdf
          â””â”€ rag_service.add_document(user_id="default", session_id="session_a", ...)

T0+420ms: Workflow B â†’ [INGEST_FILE] for file_b.pdf
          â””â”€ rag_service.add_document(user_id="default", session_id="session_b", ...)

T0+500ms: User A queries "What's in my document?"
          â”œâ”€ Searches personal RAG for user_id="default"
          â”œâ”€ Finds BOTH file_a AND file_b!  â† BUG!
          â””â”€ Answer includes User B's data!

T0+520ms: User B queries "What's in my document?"
          â”œâ”€ Searches personal RAG for user_id="default"
          â”œâ”€ Finds BOTH file_a AND file_b!  â† BUG!
          â””â”€ Answer includes User A's data!
```

**Why It Happens**:
```python
# From langgraph_workflow.py
user_id = state.get("user_id", "default")  # â† Fallback for both users!
session_id = state.get("session_id", "default")  # â† Same for both!

# At API level (app.py), user_id should come from JWT token
# BUT if extraction fails â†’ defaults to "default" for BOTH
```

---

## File Handling Edge Cases

### ğŸŸ¡ 9. PDF SPECIAL CASES

**Scenario**: Different PDF structures require different handling

| PDF Type | How It's Created | Extraction Method | Edge Case |
|----------|-----------------|------------------|-----------|
| **Native PDF** | PDF creation software | Text layer | âœ… Works |
| **Scanned PDF** | Scan â†’ PDF (image pages) | OCR (Tesseract) | âš ï¸  Quality depends on scan |
| **Mixed PDF** | Some pages text, some scanned | Both methods combined | âŒ Inconsistent results |
| **Form PDF** | PDF with form fields | Text extraction | âŒ Misses data in fields |
| **PDF/A archival** | Long-term storage format | May have encryption | âŒ Extraction fails |

**Code Analysis**:
```python
# From document_service.py
def process_file(self, file_path, ...):
    file_ext = file_path.suffix.lower().lstrip('.')
    
    if file_ext == 'pdf':
        return self._process_pdf(file_path)
```

```python
def _process_pdf(self, file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            text = ""
            for page in pdf.pages:
                # Try to extract text from text layer
                page_text = page.extract_text()
                if not page_text:
                    # âš ï¸  Falls back to OCR here
                    page_text = self._ocr_page(page)
                text += page_text
            return text
    except Exception as e:
        # Silently returns empty string or error
        logger.error(f"PDF extraction failed: {e}")
        return ""
```

**Problems**:
1. **No progress tracking**: 1000-page PDF processes silently, no feedback
2. **OCR silently degrades quality**: User doesn't know text came from OCR
3. **Performance cliff**: PDF with 1000 scanned pages takes 5+ minutes
4. **No cancellation**: Can't stop mid-extraction
5. **Memory leaks**: Large PDFs in memory, never garbage collected

---

### ğŸŸ¡ 10. EXCEL MULTIPLE SHEET HANDLING

**Scenario**: Excel file has 10 sheets, user uploads without specifying sheet

**Example Workbook**:
```
Sheet1: Financial Ratios (relevant)
Sheet2: Board Members (metadata)
Sheet3: Historical Prices (huge table, 10K rows)
Sheet4-10: Empty or deprecated
```

**Current Behavior**:
```python
# From excel_tools.py
def analyze_excel_to_markdown(file_path):
    # Likely reads all sheets or first sheet only
    df = pd.read_excel(file_path)
    return df.to_markdown()
```

**Problems**:
1. **Wrong sheet selected**: Reads first sheet, might be metadata, not data
2. **Combinatorial explosion**: 10K rows Ã— 50 columns = massive text
3. **Chunking artifacts**: Cuts table mid-row, breaks structure
4. **User confusion**: "I uploaded this file but it's not finding my data"
5. **No sheet preview**: Can't guide user to correct sheet

---

## RAG & Retrieval Edge Cases

### ğŸ”´ 11. SEMANTIC SEARCH FAILURE MODES

**Scenario**: Embeddings don't capture domain semantics

| Query | Expected | What Actually Happens |
|-------|----------|----------------------|
| "tÄƒng trÆ°á»Ÿng lá»£i nhuáº­n" (growth in profit) | Financial reports with revenue increases | Returns generic business articles |
| "Ä‘á»‹nh giÃ¡ P/E cao" (high P/E valuation) | Articles on valuation metrics | Returns articles mentioning "Ä‘á»‹nh giÃ¡" and "cao" (high) separately |
| "NgÃ¢n hÃ ng nÃ o tá»‘t nháº¥t?" (Which bank is best?) | Comparative analysis of banks | Returns individual bank pages in random order |
| "TCB so vá»›i VCB" (TCB vs VCB comparison) | Comparative articles | Returns TCB page + VCB page (no comparison) |

**Root Cause**:
```python
# From multi_collection_rag_service.py (hypothetically)
# Embeddings from: OpenAI, Gemini, or local model

# These models may not understand:
# - Vietnamese financial domain terminology
# - Implicit comparisons ("A so vá»›i B" = comparison)
# - Negations ("khÃ´ng tá»‘t" = opposite meaning)
# - Irony ("tá»‘t quÃ¡!" as sarcasm)
```

**Example Failure**:
```
Query: "CÃ´ng ty nÃ o cÃ³ doanh thu cao nháº¥t?"
       (Which company has highest revenue?)

Expected: Financial reports ranked by revenue

Actual: Returns documents with keywords:
        âœ“ "cÃ´ng ty" (company)
        âœ“ "cao" (high)
        âœ— But not necessarily highest revenue!
        âœ— Might include "CÃ´ng ty cao su" (rubber company - mismatched)
```

---

### ğŸ”´ 12. KEYWORD SEARCH BRITTLENESS

**Scenario**: Exact keyword matching fails with variations

| Query | Document Content | Match? |
|-------|------------------|--------|
| "TCB" | "Techcombank" | âŒ No |
| "Techcombank" | "TCB (Techcombank)" | âš ï¸  Partial |
| "giÃ¡ cá»• phiáº¿u" (stock price) | "GiÃ¡ cá»• phiáº¿u TCB" | âœ“ Yes |
| "giÃ¡ cá»• phiáº¿u" | "TCB's stock price" | âŒ No (English) |
| "lá»£i nhuáº­n" (profit) | "laba" (Indonesian) | âŒ No (wrong language) |
| "P/E" | "P/E Ratio" | âš ï¸  Maybe (depends on tokenizer) |

**Current Implementation**:
```python
# From multi_collection_rag_service.py (hypothetically)
def search_keyword(self, query: str, user_id: str):
    # Probably uses simple text search
    results = db.filter(
        filter={
            "user_id": user_id,
            "text": {"$contains": query}  # â† Exact substring match
        }
    )
    return results
```

**Problems**:
1. **No fuzzy matching**: Typos cause 0 results ("TCB" vs "TCb")
2. **No synonym support**: "stock" and "share" are different
3. **No lemmatization**: "invested", "investing", "investment" are different words
4. **Case sensitive**: "TCB" â‰  "tcb" in some databases
5. **Vietnamese-specific**: Tone marks matter ("tÃ i" vs "tai")

---

### ğŸŸ¡ 13. VECTOR STORE STATE INCONSISTENCY

**Scenario**: Document added to Qdrant but metadata not saved in DB

**Timeline**:
```
T0: add_document() called
T0+100ms: Document embedded and added to Qdrant âœ“
T0+200ms: Metadata saved to PostgreSQL...
T0+300ms: âŒ PostgreSQL connection drops!
          Metadata never saved
          
T0+500ms: Qdrant has vector but no metadata
          Can retrieve content but not verify source/date/user

T1 (later): 
  Vector search returns orphaned embedding
  Cannot verify user permissions (no metadata)
  Possible: Wrong user sees document from another user!
```

**Current Code**:
```python
# From file_ingestion_service.py or rag_service.py
def add_document(self, user_id, text, title, metadata):
    # Step 1: Embed and add to Qdrant
    embedding = embed_model.embed(text)
    qdrant_client.upsert(
        collection_name="documents",
        points=[Point(id=uuid4(), vector=embedding, payload=metadata)]
    )
    
    # Step 2: Save to database (âš ï¸  SEPARATE OPERATION)
    db_session.add(Document(
        user_id=user_id,
        title=title,
        content=text,
        metadata=json.dumps(metadata)
    ))
    db_session.commit()  # â† Can fail here!
```

**Problems**:
1. **No transaction**: Two separate operations, no atomic guarantee
2. **Orphaned vectors**: Qdrant has data, DB doesn't
3. **Privacy breach**: Search returns vectors user shouldn't see
4. **Inconsistent counts**: "5 documents in DB" but search finds 6
5. **No cleanup mechanism**: Orphaned vectors stay forever

---

## Tool Execution Edge Cases

### ğŸŸ¡ 14. FINANCIAL DATA STALENESS

**Scenario**: Tool returns outdated price data

**Example**:
```
Query: "What's TCB's current price?"
Time: Dec 24, 2025, 3:00 AM (before market open)

Tool: get_current_price()
â”œâ”€ Calls VnStock API
â”œâ”€ Returns: $23.50 (from yesterday's close)
â””â”€ Agent response: "TCB is trading at $23.50"  â† Stale!

Reality: Market opens at 9:15 AM, price is now $23.80
User sees: Outdated price, makes bad trading decision
```

**Reasons**:
1. **API lag**: VnStock might cache data for 5-15 minutes
2. **Market hours**: Vietnam market 9:15-11:30 AM, 1:00-3:00 PM
3. **No timestamp** in tool response, user doesn't know if fresh
4. **No cache invalidation**: Cached result served all day

**Current Code**:
```python
# From vnstock_tools.py
def get_current_price(ticker):
    try:
        # VnStock API call - no freshness guarantee
        price = vnstock.stock.get_price(ticker)
        return {"price": price}
    except Exception:
        return {"error": "Price not available"}
```

**Problems**:
1. **No timestamp**: Response doesn't say when price was fetched
2. **No staleness check**: Accepts any data regardless of age
3. **Market hours unaware**: Returns yesterday's close at 3 AM
4. **No refresh on error**: Retries with same stale cache

---

### ğŸŸ¡ 15. MISSING OR INVALID TICKER HANDLING

**Scenario**: Tool receives invalid or delisted ticker

| Ticker | Status | Tool Behavior | Outcome |
|--------|--------|---------------|---------|
| "TCB" | Valid, active | Returns data | âœ“ Works |
| "tcb" | Valid (lowercase) | May return 404 | âŒ Or normalizes to TCB |
| "TCB!" | Invalid (special char) | Returns error | âœ“ Error message |
| "DEADCO" | Delisted 5 years ago | API returns 404 | âŒ Or empty history |
| "" | Empty string | Tool behavior unclear | âŒ Undefined |
| "TCB TCB" | Duplicate | API interprets as typo | âŒ Wrong interpretation |
| "A" | Too short, real ticker? | Ambiguous | âŒ Matches multiple |

**Code Issues**:
```python
# From vnstock_tools.py or technical_tools.py
def calculate_sma(ticker, window=20, days=100):
    # âš ï¸  No validation of ticker format
    history = vnstock.stock.get_historical_data(
        ticker,  # Could be invalid!
        start_date=start_date,
        end_date=end_date
    )
    
    if not history:
        # What to do? Return error? Empty result?
        return {"error": "No data found"}
    
    # Calculate SMA
    return {"sma": sma_values}
```

**Problems**:
1. **No format validation**: "-TCB", "TCB.HNX", "TCB:VNM" treated as different
2. **No typo correction**: "TVC" vs "TCB" misses by 1 letter
3. **Silent failure**: Empty result looks like "no data available"
4. **User confusion**: "I asked for TCB but got no results"

---

## State Management Edge Cases

### ğŸ”´ 16. BIDIRECTIONAL STATE MUTATION

**Scenario**: Nodes modify state in unexpected ways, affecting other nodes

**Example Flow**:
```
Initial State:
â”œâ”€â”€ best_search_results: [doc1, doc2, doc3]
â””â”€â”€ conversation_history: [HumanMessage, AIMessage]

Node: Agent
â”œâ”€â”€ Reads: best_search_results
â”œâ”€â”€ Reads: conversation_history
â”œâ”€â”€ âš ï¸  MUTATES: conversation_history.append(new_message)
â””â”€â”€ Returns: updated state

Node: Tools (next node)
â”œâ”€â”€ Reads: conversation_history (has new_message!)
â”œâ”€â”€ UNEXPECTED: Processes message it didn't create
â””â”€â”€ Bug: Potential duplicate processing
```

**Code Risk**:
```python
# From langgraph_workflow.py
async def node_agent(self, state: WorkflowState) -> Dict[str, Any]:
    messages = state.get("conversation_history", [])
    
    # âš ï¸  Direct mutation of list
    updated_messages = messages + [response]  # OK - creates new list
    # BUT if code later does:
    messages.append(response)  # âŒ Direct mutation of shared reference!
    
    return {"conversation_history": updated_messages}
```

**Why It's Dangerous**:
```
If messages is a reference to the original state dict's list:
- Modifying messages modifies the state directly
- Other nodes see the modified state
- Can't rollback if error occurs after mutation
- Hard to debug: "Where did this message come from?"
```

---

### ğŸŸ¡ 17. TYPE MISMATCHES IN STATE TRANSITIONS

**Scenario**: State expects Dict but receives List, or None when non-None expected

| Field | Type Spec | Possible Runtime Values | Issue |
|-------|-----------|------------------------|-------|
| `extracted_file_data` | `Optional[Dict]` | `None`, `{}`, `Dict[str, Dict]` | âœ“ OK |
| `ingested_file_ids` | `List[str]` | `[]`, `["file1"]`, `None`! | âŒ None violates type |
| `best_search_results` | `List[Dict]` | `[]`, `[{...}]`, `"error message"`! | âŒ String instead of List |
| `conversation_history` | `List[Dict]` | `[]`, `[...messages...]`, `None` | âŒ None breaks iteration |
| `metadata` | `Dict` | `{}`, `{...}`, `None` | âš ï¸  May be None |

**Example Code Bug**:
```python
# From node_ingest_file
ingested_file_ids = ingested_file_ids if isinstance(ingested_file_ids, list) else []

# Later, from node_agent
for file_id in state.get("ingested_file_ids"):  # â† Could be None!
    # Crashes here if None
    search_in_file(file_id)  # TypeError: 'NoneType' object is not iterable
```

---

## Error Handling Edge Cases

### ğŸ”´ 18. SILENT FAILURES IN TRY-EXCEPT BLOCKS

**Scenario**: Errors are caught but not properly handled

**Pattern Found Repeatedly**:
```python
# From langgraph_workflow.py, file_ingestion_service.py, etc.
try:
    result = risky_operation()
    return {"success": True, "data": result}
except Exception as e:
    logger.error(f"Operation failed: {e}")  # â† Logged but...
    return {"success": False, "error": str(e)}  # â† Graceful fallback
    # âŒ Continue processing as if success!
```

**Why It's Dangerous**:
```
User uploads file:
â”œâ”€ [EXTRACT_DATA] catches exception, returns {"success": False, "error": "..."}
â”œâ”€ [INGEST_FILE] checks `if not extracted_data` â†’ skips ingestion âœ“
â”œâ”€ [AGENT] proceeds without extracted data âœ“
â””â”€ User gets answer without their document â†’ No notification!

User never knows:
- File upload failed
- Why it failed
- What to do differently
```

**Better Pattern**:
```python
try:
    result = risky_operation()
    return result
except FileNotFoundError as e:
    # Specific handling for known errors
    logger.warning(f"File missing: {e}")
    return None  # Caller knows None = file missing
except ValueError as e:
    # Validation error - user's fault
    logger.warning(f"Invalid input: {e}")
    raise  # Re-raise for API to catch and return 400
except Exception as e:
    # Unexpected error - system's fault
    logger.error(f"Unexpected error: {e}", exc_info=True)
    raise  # Re-raise for API to catch and return 500
```

---

### ğŸŸ¡ 19. TIMEOUT & RESOURCE EXHAUSTION

**Scenario**: Long-running operations exhaust resources

| Operation | Timeout | Impact | Severity |
|-----------|---------|--------|----------|
| **PDF extraction (1000 pages)** | 5 min? | Extraction hangs | ğŸŸ¡ MEDIUM |
| **OCR on 100 images** | 10 min? | Tesseract uses 100% CPU | ğŸ”´ HIGH |
| **Vector embedding 50K chunks** | 30 min? | Database connection pool exhausted | ğŸ”´ HIGH |
| **Semantic search (100K vectors)** | 10 sec? | Qdrant query timeout | ğŸŸ¡ MEDIUM |
| **LLM inference (128K tokens)** | 2 min? | API timeout | ğŸ”´ HIGH |

**Current State**:
- No timeouts configured âŒ
- No resource limits âŒ
- No async/queue system for heavy ops âŒ
- No progress tracking âŒ
- No cancellation mechanism âŒ

**Example**:
```python
# From file_processing_pipeline.py
def process(self, file_path, file_type, file_name):
    # âš ï¸  No timeout set
    if file_type == "pdf":
        return self._process_pdf(file_path)  # â† Could hang forever!

def _process_pdf(self, file_path):
    # âš ï¸  No OCR timeout
    text = pytesseract.image_to_string(image)  # â† 10 min+ for complex images
```

---

### ğŸŸ¡ 20. EXCEPTION INFORMATION LOSS

**Scenario**: Error details lost in exception translation

**Example**:
```
VnStock API error:
â”œâ”€ Original exception: "Connection timeout after 30s (service down)"
â”œâ”€ Tool catches: except Exception as e
â”œâ”€ Translates to: {"error": "Tool execution failed"}  â† Lost detail!
â”œâ”€ Agent sees: {"error": "Tool execution failed"}  â† No retry logic!
â”œâ”€ User sees: "I couldn't get that data"  â† No context!
â””â”€ Root cause never fixed: Service still down 1 hour later

vs. Better:
â”œâ”€ Tool catches specific: except requests.Timeout
â”œâ”€ Translates to: {"error": "Service temporarily unavailable", "retry_after": 60}
â”œâ”€ Agent sees: retry flag â†’ decides to wait and retry
â”œâ”€ User sees: "Getting fresh data..." â†’ waits
â””â”€ Data fetched successfully after wait
```

---

## Performance & Scalability Edge Cases

### ğŸŸ¡ 21. VECTOR STORE SCALING LIMITS

**Scenario**: Qdrant performance degrades with collection size

| Collection Size | Vector Search Latency | Issue |
|-----------------|----------------------|-------|
| 1K documents | 10ms | âœ“ Fine |
| 100K documents | 100ms | âœ“ Acceptable |
| 1M documents | 1s | ğŸŸ¡ Noticeable |
| 10M documents | 10s | ğŸ”´ Too slow |
| 100M documents | > 60s | ğŸ”´ Timeout |

**Why It Happens**:
```
- Vector search requires distance calculation with every vector
- With 10M vectors, even simple operations slow down
- Filtering (by user_id) helps, but doesn't scale linearly
- Qdrant needs manual sharding/partitioning for multi-tenancy
```

**Current Risk**:
```
After 1 year of operation:
â”œâ”€â”€ 1000 users Ã— 100 documents each = 100K documents
â”œâ”€â”€ Semantic search takes 500ms â†’ User sees delay
â”œâ”€â”€ Add more documents â†’ 200ms search becomes 1s
â”œâ”€â”€ Users complain â†’ App seems broken
â””â”€â”€ No built-in sharding â†’ Can't distribute load
```

---

### ğŸŸ¡ 22. EMBEDDING MODEL CAPACITY

**Scenario**: Too many concurrent embedding operations

**Example**:
```
10 users upload 10 files simultaneously:
â”œâ”€â”€ 100 files being processed
â”œâ”€â”€ Each file â†’ 50 chunks
â”œâ”€â”€ 5000 chunks need embedding
â”œâ”€â”€ Embedding API limit: 100 req/min
â””â”€â”€ Queuing time: 5000 / 100 = 50 minutes!

Result:
â”œâ”€â”€ User 1 waits 50 minutes for "upload complete"
â”œâ”€â”€ UI shows spinning wheel
â”œâ”€â”€ User refreshes browser (cancels upload)
â”œâ”€â”€ Partial data in database
â””â”€â”€ Orphaned vectors in Qdrant
```

**Current Implementation**:
```python
# From file_ingestion_service.py
for chunk in chunks:
    embedding = embed_model.embed(chunk)  # â† Sequential, no batching!
    qdrant_client.upsert(...)
    # Process one chunk per iteration = slow
```

---

## Recommendation Matrix

### ğŸ”´ CRITICAL (Fix Immediately)

| #  | Issue | Impact | Effort | Priority |
|----|-------|--------|--------|----------|
| 1  | User isolation defaults to "default" | Data breach | Medium | P0 |
| 3  | Tool errors cause infinite retries | API hang | Medium | P0 |
| 4  | RAG vs Tool result conflicts unresolved | Wrong answers | High | P0 |
| 11 | Semantic search fails on domain terms | Poor search | Medium | P0 |
| 16 | State mutations affect other nodes | Race conditions | High | P0 |
| 18 | Silent failures in try-catch | Lost user data | Medium | P0 |

### ğŸŸ¡ HIGH (Fix in Sprint)

| #  | Issue | Impact | Effort | Priority |
|----|-------|--------|--------|----------|
| 2  | No file validation | Data waste | Low | P1 |
| 6  | Encoding/Unicode issues | Search fails | Medium | P1 |
| 7  | Token limit overflow | Chat crash | High | P1 |
| 8  | Race conditions in concurrent requests | Data leak | High | P1 |
| 9  | PDF special cases | Poor extraction | Medium | P1 |
| 12 | Keyword search brittleness | No results | Medium | P1 |
| 13 | Vector store inconsistency | Data loss | High | P1 |
| 14 | Financial data staleness | Wrong decisions | Medium | P1 |
| 17 | Type mismatches in state | Crashes | Medium | P1 |
| 19 | Timeouts/resource exhaustion | API hang | High | P1 |
| 21 | Vector store scaling | Slowdown | High | P2 |

### ğŸŸ¢ MEDIUM (Plan for Next Phase)

| #  | Issue | Impact | Effort | Priority |
|----|-------|--------|--------|----------|
| 5  | Circular tool dependencies | Wrong results | High | P2 |
| 10 | Excel multiple sheets | User confusion | Low | P2 |
| 15 | Invalid ticker handling | Wrong ticker | Low | P2 |
| 20 | Exception info loss | Poor debugging | Low | P2 |
| 22 | Embedding capacity | Slow uploads | High | P3 |

---

## Implementation Checklist

### For Each Edge Case Fix

- [ ] **Add input validation**
  ```python
  if not user_id:
      raise ValueError("user_id required")
  ```

- [ ] **Add type checking**
  ```python
  from typing import List
  assert isinstance(results, list), f"Expected list, got {type(results)}"
  ```

- [ ] **Add logging**
  ```python
  logger.warning(f"Edge case detected: {condition}")
  ```

- [ ] **Add unit test**
  ```python
  def test_edge_case_empty_input():
      result = function_under_test("")
      assert result == expected_behavior
  ```

- [ ] **Add integration test**
  ```python
  def test_end_to_end_with_edge_case():
      response = api_call(edge_case_input)
      assert response.status_code == expected_code
  ```

- [ ] **Update documentation**
  - Known limitations
  - Workarounds
  - Future improvements

---

## Summary

This Financial Agent has a sophisticated architecture but exhibits common edge case vulnerabilities:

**Most Critical**: User isolation, state mutations, and tool error handling need immediate fixes.

**Most Common**: Encoding issues, file handling, and type mismatches appear throughout the codebase.

**Most Impactful**: RAG search and tool execution directly affect end-user experience.

**Next Steps**:
1. Implement user ID validation at all entry points
2. Add proper error boundaries with rollback
3. Implement token counting before LLM calls
4. Add resource limits and timeouts
5. Comprehensive test suite for edge cases

---

**Document Generated**: December 24, 2025  
**Status**: Analysis Complete  
**Recommendations**: Prioritized by severity and effort
