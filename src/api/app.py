"""
FastAPI Application - REST API cho Financial Agent
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List
import logging
import asyncio
import os
import tempfile
from pathlib import Path

from ..agent import FinancialAgent
from ..tools.financial_report_tools import analyze_financial_report
from ..tools.excel_tools import analyze_excel_to_markdown
from ..tools.pdf_tools import analyze_pdf

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI app
app = FastAPI(
    title="Financial Agent API",
    description="API cho Agent t∆∞ v·∫•n ƒë·∫ßu t∆∞ ch·ª©ng kho√°n Vi·ªát Nam",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize agent (lazy loading)
agent = None


def get_agent():
    """Get or create agent instance"""
    global agent
    if agent is None:
        logger.info("Initializing Financial Agent...")
        agent = FinancialAgent()
    return agent


# Request/Response models
class ChatRequest(BaseModel):
    """Request model for chat endpoint"""
    question: str = Field(
        ..., 
        description="C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng (ti·∫øng Vi·ªát)",
        min_length=1,
        max_length=500
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "question": "Cho t√¥i bi·∫øt th√¥ng tin v·ªÅ c√¥ng ty VNM?"
            }
        }


class ChatResponse(BaseModel):
    """Response model for chat endpoint"""
    answer: str = Field(..., description="C√¢u tr·∫£ l·ªùi t·ª´ Agent")
    
    class Config:
        json_schema_extra = {
            "example": {
                "answer": "VNM l√† m√£ ch·ª©ng kho√°n c·ªßa C√¥ng ty C·ªï ph·∫ßn S·ªØa Vi·ªát Nam (Vinamilk)..."
            }
        }


class FileUploadRequest(BaseModel):
    """Request model for file upload endpoint"""
    question: Optional[str] = Field(
        default="",
        description="C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m v·ªÅ file"
    )


class FileAnalysisResponse(BaseModel):
    """Response model for file analysis"""
    success: bool
    report_type: str
    extracted_text: str
    analysis: str
    message: str


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Financial Agent API - Vietnamese Stock Market Assistant",
        "version": "1.0.0",
        "endpoints": {
            "chat": "POST /api/chat - Main chat endpoint",
            "health": "GET /health - Health check",
            "docs": "GET /docs - API documentation (Swagger UI)",
            "redoc": "GET /redoc - API documentation (ReDoc)"
        },
        "capabilities": [
            "Tra c·ª©u th√¥ng tin c√¥ng ty (get_company_info)",
            "D·ªØ li·ªáu gi√° l·ªãch s·ª≠ (get_historical_data)",
            "T√≠nh SMA - Simple Moving Average (calculate_sma)",
            "T√≠nh RSI - Relative Strength Index (calculate_rsi)"
        ]
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Check if agent can be initialized
        agent_instance = get_agent()
        return {
            "status": "healthy",
            "agent_ready": agent_instance is not None,
            "tools_count": len(agent_instance.tools) if agent_instance else 0
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e)
        }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint - Nh·∫≠n c√¢u h·ªèi v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi
    
    Args:
        request: ChatRequest v·ªõi field 'question'
        
    Returns:
        ChatResponse v·ªõi field 'answer'
        
    Example:
        POST /api/chat
        {
            "question": "Th√¥ng tin v·ªÅ c√¥ng ty VNM?"
        }
        
        Response:
        {
            "answer": "VNM l√† C√¥ng ty C·ªï ph·∫ßn S·ªØa Vi·ªát Nam..."
        }
    """
    try:
        logger.info(f"Received question: {request.question}")
        
        # Get agent instance
        agent_instance = get_agent()
        
        # Process question using agent
        answer = await agent_instance.aquery(request.question)
        
        logger.info(f"Answer generated successfully (length: {len(answer)})")
        
        return ChatResponse(answer=answer)
    except Exception as e:
        logger.error(f"Error processing chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
        return ChatResponse(answer=answer)
        
    except Exception as e:
        logger.error(f"Error processing question: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
        )


@app.get("/api/test")
async def test_endpoint():
    """Test endpoint to verify API is working"""
    return {
        "status": "ok",
        "message": "API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng",
        "sample_questions": [
            "Cho t√¥i bi·∫øt th√¥ng tin v·ªÅ c√¥ng ty VNM?",
            "Gi√° VCB trong 3 th√°ng g·∫ßn nh·∫•t?",
            "T√≠nh SMA 20 ng√†y cho HPG",
            "RSI c·ªßa VIC hi·ªán t·∫°i?"
        ],
        "usage": {
            "endpoint": "POST /api/chat",
            "body": {
                "question": "your question here"
            }
        }
    }


@app.on_event("startup")
async def startup_event():
    """Run on application startup"""
    logger.info("=" * 60)
    logger.info("Financial Agent API Starting...")
    logger.info("=" * 60)
    
    # Pre-initialize agent
    try:
        get_agent()
        logger.info("‚úÖ Agent initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize agent: {e}")
    
    logger.info("üöÄ API ready to accept requests")


@app.post("/api/upload", response_model=FileAnalysisResponse)
async def upload_file(
    file: UploadFile = File(...),
    question: str = Form(default="")
):
    """
    Upload v√† ph√¢n t√≠ch file (·∫£nh b√°o c√°o t√†i ch√≠nh ho·∫∑c Excel)
    
    Args:
        file: File ·∫£nh (PNG, JPG, PDF) ho·∫∑c Excel (.xlsx, .xls)
        question: C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m (optional)
        
    Returns:
        FileAnalysisResponse v·ªõi k·∫øt Qu·∫£Analysis
        
    Example:
        POST /api/upload
        Form data:
        - file: [binary file]
        - question: "Ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh n√†y"
    """
    temp_path = None
    try:
        file_extension = Path(file.filename).suffix.lower()
        
        # Ki·ªÉm tra lo·∫°i file
        image_types = ["image/png", "image/jpeg", "image/jpg"]
        pdf_types = ["application/pdf"]
        excel_types = [
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-excel",
            "application/excel"
        ]
        
        is_image = file.content_type in image_types or file_extension in ['.png', '.jpg', '.jpeg']
        is_pdf = file.content_type in pdf_types or file_extension == '.pdf'
        is_excel = file.content_type in excel_types or file_extension in ['.xlsx', '.xls']
        
        if not (is_image or is_pdf or is_excel):
            raise HTTPException(
                status_code=400,
                detail=f"Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. H·ªó tr·ª£: PNG, JPG, PDF, XLSX, XLS. Nh·∫≠n ƒë∆∞·ª£c: {file.content_type}"
            )
        
        # L∆∞u file t·∫°m
        logger.info(f"Processing file: {file.filename}")
        
        with tempfile.NamedTemporaryFile(
            delete=False,
            suffix=file_extension,
            dir=tempfile.gettempdir()
        ) as temp_file:
            temp_path = temp_file.name
            content = await file.read()
            temp_file.write(content)
            logger.info(f"File saved to: {temp_path}")
        
        # X·ª≠ l√Ω theo lo·∫°i file
        if is_pdf:
            logger.info("Processing PDF file...")
            result = analyze_pdf(temp_path, question)
            
            if not result.success:
                raise HTTPException(
                    status_code=400,
                    detail=f"L·ªói x·ª≠ l√Ω PDF: {result.message}"
                )
            
            # K·∫øt h·ª£p extracted text v√† tables markdown
            combined_data = f"Extracted Text:\n{result.extracted_text}\n\nTables:\n{result.tables_markdown}"
            
            # G·ª≠i d·ªØ li·ªáu PDF cho Gemini ph√¢n t√≠ch n·∫øu c√≥ c√¢u h·ªèi
            analysis = result.analysis
            if question.strip():
                try:
                    from src.llm.llm_factory import LLMFactory
                    
                    logger.info("Sending PDF data to Gemini for analysis...")
                    
                    # ƒê·ªçc system prompt
                    system_prompt_path = Path(__file__).parent.parent / "agent" / "prompts" / "financial_report_prompt.txt"
                    if system_prompt_path.exists():
                        with open(system_prompt_path, 'r', encoding='utf-8') as f:
                            base_system_prompt = f.read()
                    else:
                        base_system_prompt = "B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh chuy√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh doanh nghi·ªáp. H√£y ph√¢n t√≠ch d·ªØ li·ªáu sau m·ªôt c√°ch chi ti·∫øt, chuy√™n s√¢u v√† ƒë∆∞a ra nh·ªØng insights qu√Ω gi√° cho nh√† ƒë·∫ßu t∆∞."
                    
                    # K·∫øt h·ª£p system prompt + user question
                    system_prompt = f"{base_system_prompt}\n\n---\n\nY√äU C·∫¶U T·ª™ NG∆Ø·ªúI D√ôNG:\n{question}"
                    
                    # G·ªçi Gemini
                    llm = LLMFactory.get_llm()
                    from langchain_core.messages import SystemMessage, HumanMessage
                    
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=f"Ph√¢n t√≠ch d·ªØ li·ªáu PDF sau:\n\n{combined_data}")
                    ]
                    
                    response = await llm.ainvoke(messages)
                    analysis = response.content
                    
                    logger.info("‚úì Gemini analysis completed for PDF")
                    
                except Exception as e:
                    logger.warning(f"Error sending PDF to Gemini: {e}")
                    analysis = result.analysis
            
            return FileAnalysisResponse(
                success=result.success,
                report_type=f"PDF Document ({result.processing_method.upper()})",
                extracted_text=result.extracted_text[:500] + "..." if len(result.extracted_text) > 500 else result.extracted_text,
                analysis=analysis,
                message=f"X·ª≠ l√Ω PDF th√†nh c√¥ng. Method: {result.processing_method}"
            )
        
        elif is_excel:
            logger.info("Processing Excel file...")
            result = analyze_excel_to_markdown(temp_path)
            markdown_output = result.get('markdown', '')
            analysis = markdown_output
            
            # G·ª≠i Markdown cho Gemini ph√¢n t√≠ch v·ªõi system prompt t√πy ch·ªânh
            try:
                from src.llm.llm_factory import LLMFactory
                
                logger.info("Sending Excel data to Gemini for analysis...")
                
                # ƒê·ªçc system prompt m·∫∑c ƒë·ªãnh t·ª´ file
                system_prompt_path = Path(__file__).parent.parent / "agent" / "prompts" / "excel_analysis_prompt.txt"
                if system_prompt_path.exists():
                    with open(system_prompt_path, 'r', encoding='utf-8') as f:
                        base_system_prompt = f.read()
                else:
                    base_system_prompt = "B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh chuy√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh doanh nghi·ªáp. H√£y ph√¢n t√≠ch d·ªØ li·ªáu sau m·ªôt c√°ch chi ti·∫øt, chuy√™n s√¢u v√† ƒë∆∞a ra nh·ªØng insights qu√Ω gi√° cho nh√† ƒë·∫ßu t∆∞."
                
                # K·∫øt h·ª£p system prompt m·∫∑c ƒë·ªãnh + user instruction
                if question.strip():
                    system_prompt = f"{base_system_prompt}\n\n---\n\nY√äU C·∫¶U T·ª™ NG∆Ø·ªúI D√ôNG:\n{question}"
                else:
                    system_prompt = base_system_prompt
                
                # G·ªçi Gemini tr·ª±c ti·∫øp
                llm = LLMFactory.get_llm()
                from langchain_core.messages import SystemMessage, HumanMessage
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=f"Ph√¢n t√≠ch d·ªØ li·ªáu t√†i ch√≠nh sau:\n\n{markdown_output}")
                ]
                
                response = await llm.ainvoke(messages)
                analysis = response.content
                
                logger.info("‚úì Gemini analysis completed")
                
            except Exception as e:
                logger.error(f"Error sending to Gemini: {e}")
                analysis = markdown_output
            
            return FileAnalysisResponse(
                success=result['success'],
                report_type="Excel Financial Data",
                extracted_text=markdown_output,
                analysis=analysis,
                message=result['message']
            )
        
        else:  # is_image
            logger.info("Processing image file...")
            result = analyze_financial_report(temp_path)
            analysis = result.analysis
            extracted_text = result.extracted_text
            report_type = result.report_type
            
            try:
                from src.llm.llm_factory import LLMFactory
                from langchain_core.messages import SystemMessage, HumanMessage
                
                llm = LLMFactory.get_llm()
                
                # Determine how to process the image based on context
                if question.strip():
                    # User provided a specific question - use it as context
                    logger.info("User provided question - processing image with context...")
                    
                    # Combine the extracted text with the user question
                    # Let the agent decide how to handle it (financial analysis or regular question)
                    agent_instance = get_agent()
                    
                    # Combine extracted text with user question
                    combined_query = f"T√¥i ƒë√£ upload m·ªôt ·∫£nh c√≥ n·ªôi dung sau:\n\n{extracted_text}\n\n---\n\nC√¢u h·ªèi c·ªßa t√¥i l√†: {question}"
                    
                    # Use the financial agent to handle the question
                    analysis = await agent_instance.aquery(combined_query)
                    
                    logger.info("‚úì Agent processed image with user question")
                else:
                    # No user question provided - check if content looks like a financial report or a general question
                    logger.info("No question provided - analyzing image content type...")
                    
                    # Use LLM to determine content type and extract question if any
                    classification_prompt = f"""Analyze this extracted text from an image and determine:
1. Is this a financial report/statement? (yes/no)
2. If not, what type of content is it? (e.g., screenshot of question, chart, document, etc.)
3. If it contains a question, extract it.

Extracted text:
{extracted_text}

Respond in JSON format:
{{"is_financial_report": true/false, "content_type": "...", "question": "..." (if any)}}"""
                    
                    classification_response = await llm.ainvoke([
                        HumanMessage(content=classification_prompt)
                    ])
                    
                    try:
                        import json as json_module
                        classification = json_module.loads(classification_response.content)
                        is_financial = classification.get("is_financial_report", False)
                        extracted_question = classification.get("question", "")
                    except:
                        # Default to financial report if classification fails
                        is_financial = True
                        extracted_question = ""
                    
                    # Process based on content type
                    if is_financial:
                        # Process as financial report
                        logger.info("Content identified as financial report - using financial analysis")
                        system_prompt_path = Path(__file__).parent.parent / "agent" / "prompts" / "financial_report_prompt.txt"
                        if system_prompt_path.exists():
                            with open(system_prompt_path, 'r', encoding='utf-8') as f:
                                base_system_prompt = f.read()
                        else:
                            base_system_prompt = "B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh chuy√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh doanh nghi·ªáp. H√£y ph√¢n t√≠ch d·ªØ li·ªáu sau m·ªôt c√°ch chi ti·∫øt, chuy√™n s√¢u v√† ƒë∆∞a ra nh·ªØng insights qu√Ω gi√° cho nh√† ƒë·∫ßu t∆∞."
                        
                        system_prompt = base_system_prompt
                        messages = [
                            SystemMessage(content=system_prompt),
                            HumanMessage(content=f"Ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ b√°o c√°o t√†i ch√≠nh (ƒë√£ OCR):\n\n{extracted_text}")
                        ]
                        response = await llm.ainvoke(messages)
                        analysis = response.content
                        report_type = "Financial Report (Image)"
                    else:
                        # Process as general question or content
                        logger.info("Content identified as non-financial - using general agent")
                        agent_instance = get_agent()
                        
                        if extracted_question:
                            # If a question was extracted, answer it with the context
                            combined_query = f"D·ª±a tr√™n n·ªôi dung ·∫£nh sau:\n\n{extracted_text}\n\n---\n\nC√¢u h·ªèi: {extracted_question}"
                            analysis = await agent_instance.aquery(combined_query)
                        else:
                            # Just provide information about the content
                            combined_query = f"ƒê√¢y l√† n·ªôi dung t·ª´ ·∫£nh ƒë∆∞·ª£c OCR:\n\n{extracted_text}\n\nH√£y t√≥m t·∫Øt ho·∫∑c ph√¢n t√≠ch n·ªôi dung n√†y."
                            analysis = await agent_instance.aquery(combined_query)
                        
                        report_type = f"Image ({classification.get('content_type', 'Unknown')})"
                    
                    logger.info("‚úì Image processing completed")
                
            except Exception as e:
                logger.warning(f"Error in intelligent image processing: {e}")
                # Fallback to original analysis
                analysis = result.analysis
            
            return FileAnalysisResponse(
                success=result.success,
                report_type=report_type,
                extracted_text=extracted_text[:500] + "..." if len(extracted_text) > 500 else extracted_text,
                analysis=analysis,
                message=result.message
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}"
        )
    finally:
        # X√≥a file t·∫°m
        if temp_path:
            try:
                if os.path.exists(temp_path):
                    # Th·ª≠ x√≥a file
                    os.remove(temp_path)
                    logger.info(f"Temp file deleted: {temp_path}")
            except PermissionError:
                # File ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng - schedule x√≥a sau
                try:
                    import atexit
                    atexit.register(lambda: os.remove(temp_path) if os.path.exists(temp_path) else None)
                    logger.debug(f"Scheduled temp file cleanup: {temp_path}")
                except Exception as cleanup_err:
                    logger.warning(f"Could not schedule cleanup: {cleanup_err}")
            except Exception as e:
                logger.warning(f"Error deleting temp file: {e}")


class ExcelAnalysisResponse(BaseModel):
    """Excel analysis response model"""
    success: bool
    file_name: str
    sheet_count: int
    sheet_names: List[str]
    markdown: str
    analysis: Optional[str] = None
    message: str


@app.post("/api/upload-excel", response_model=ExcelAnalysisResponse)
async def upload_excel_file(
    file: UploadFile = File(...),
    question: str = Form(default="")
):
    """
    Upload v√† ph√¢n t√≠ch file Excel (d·ªØ li·ªáu t√†i ch√≠nh)
    
    Args:
        file: File Excel (.xlsx, .xls)
        question: C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m (optional)
        
    Returns:
        ExcelAnalysisResponse v·ªõi Markdown v√† ph√¢n t√≠ch
        
    Example:
        POST /api/upload-excel
        Form data:
        - file: [binary excel file]
        - question: "Ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh n√†y"
    """
    temp_path = None
    try:
        # Ki·ªÉm tra lo·∫°i file
        allowed_types = [
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-excel",
            "application/excel"
        ]
        if file.content_type not in allowed_types:
            raise HTTPException(
                status_code=400,
                detail=f"Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. H·ªó tr·ª£: .xlsx, .xls. Nh·∫≠n ƒë∆∞·ª£c: {file.content_type}"
            )
        
        # L∆∞u file t·∫°m
        logger.info(f"Processing Excel file: {file.filename}")
        
        with tempfile.NamedTemporaryFile(
            delete=False,
            suffix=Path(file.filename).suffix,
            dir=tempfile.gettempdir()
        ) as temp_file:
            temp_path = temp_file.name
            content = await file.read()
            temp_file.write(content)
            logger.info(f"File saved to: {temp_path}")
        
        # Ph√¢n t√≠ch Excel th√†nh Markdown
        logger.info("Converting Excel to Markdown...")
        result = analyze_excel_to_markdown(temp_path)
        
        # N·∫øu c√≥ c√¢u h·ªèi, g·ª≠i Markdown cho Gemini/Agent x·ª≠ l√Ω
        analysis = ""
        if question.strip():
            try:
                agent_instance = get_agent()
                combined_question = f"{question}\n\nD·ªØ li·ªáu t√†i ch√≠nh (Markdown):\n\n{result['markdown']}"
                answer = await agent_instance.aquery(combined_question)
                analysis = answer
                logger.info("‚úì Agent analysis completed")
            except Exception as e:
                logger.warning(f"Error sending to agent: {e}")
                analysis = ""
        
        return ExcelAnalysisResponse(
            success=result['success'],
            file_name=result['file_name'],
            sheet_count=result['sheet_count'],
            sheet_names=result['sheet_names'],
            markdown=result['markdown'],
            analysis=analysis,
            message=result['message']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing Excel file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"L·ªói x·ª≠ l√Ω file Excel: {str(e)}"
        )
    finally:
        # X√≥a file t·∫°m
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                logger.info(f"Temp file deleted: {temp_path}")
            except Exception as e:
                logger.warning(f"Error deleting temp file: {e}")


@app.on_event("shutdown")
async def shutdown_event():
    """Run on application shutdown"""
    logger.info("Shutting down Financial Agent API...")
