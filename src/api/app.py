"""
FastAPI Application - REST API cho Financial Agent
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List
import logging
import asyncio
import os
import tempfile
from pathlib import Path

from ..agent import FinancialAgent
from ..tools.financial_report_tools import analyze_financial_report
from ..tools.excel_tools import analyze_excel_to_markdown

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI app
app = FastAPI(
    title="Financial Agent API",
    description="API cho Agent t∆∞ v·∫•n ƒë·∫ßu t∆∞ ch·ª©ng kho√°n Vi·ªát Nam",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize agent (lazy loading)
agent = None


def get_agent():
    """Get or create agent instance"""
    global agent
    if agent is None:
        logger.info("Initializing Financial Agent...")
        agent = FinancialAgent()
    return agent


# Request/Response models
class ChatRequest(BaseModel):
    """Request model for chat endpoint"""
    question: str = Field(
        ..., 
        description="C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng (ti·∫øng Vi·ªát)",
        min_length=1,
        max_length=500
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "question": "Cho t√¥i bi·∫øt th√¥ng tin v·ªÅ c√¥ng ty VNM?"
            }
        }


class ChatResponse(BaseModel):
    """Response model for chat endpoint"""
    answer: str = Field(..., description="C√¢u tr·∫£ l·ªùi t·ª´ Agent")
    
    class Config:
        json_schema_extra = {
            "example": {
                "answer": "VNM l√† m√£ ch·ª©ng kho√°n c·ªßa C√¥ng ty C·ªï ph·∫ßn S·ªØa Vi·ªát Nam (Vinamilk)..."
            }
        }


class FileUploadRequest(BaseModel):
    """Request model for file upload endpoint"""
    question: Optional[str] = Field(
        default="",
        description="C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m v·ªÅ file"
    )


class FileAnalysisResponse(BaseModel):
    """Response model for file analysis"""
    success: bool
    report_type: str
    extracted_text: str
    analysis: str
    message: str


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Financial Agent API - Vietnamese Stock Market Assistant",
        "version": "1.0.0",
        "endpoints": {
            "chat": "POST /api/chat - Main chat endpoint",
            "health": "GET /health - Health check",
            "docs": "GET /docs - API documentation (Swagger UI)",
            "redoc": "GET /redoc - API documentation (ReDoc)"
        },
        "capabilities": [
            "Tra c·ª©u th√¥ng tin c√¥ng ty (get_company_info)",
            "D·ªØ li·ªáu gi√° l·ªãch s·ª≠ (get_historical_data)",
            "T√≠nh SMA - Simple Moving Average (calculate_sma)",
            "T√≠nh RSI - Relative Strength Index (calculate_rsi)"
        ]
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Check if agent can be initialized
        agent_instance = get_agent()
        return {
            "status": "healthy",
            "agent_ready": agent_instance is not None,
            "tools_count": len(agent_instance.tools) if agent_instance else 0
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e)
        }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Chat endpoint - Nh·∫≠n c√¢u h·ªèi v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi
    
    Args:
        request: ChatRequest v·ªõi field 'question'
        
    Returns:
        ChatResponse v·ªõi field 'answer'
        
    Example:
        POST /api/chat
        {
            "question": "Th√¥ng tin v·ªÅ c√¥ng ty VNM?"
        }
        
        Response:
        {
            "answer": "VNM l√† C√¥ng ty C·ªï ph·∫ßn S·ªØa Vi·ªát Nam..."
        }
    """
    try:
        logger.info(f"Received question: {request.question}")
        
        # Get agent instance
        agent_instance = get_agent()
        
        # Process question using agent
        answer = await agent_instance.aquery(request.question)
        
        logger.info(f"Answer generated successfully (length: {len(answer)})")
        
        return ChatResponse(answer=answer)
    except Exception as e:
        logger.error(f"Error processing chat: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
        return ChatResponse(answer=answer)
        
    except Exception as e:
        logger.error(f"Error processing question: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
        )


@app.get("/api/test")
async def test_endpoint():
    """Test endpoint to verify API is working"""
    return {
        "status": "ok",
        "message": "API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng",
        "sample_questions": [
            "Cho t√¥i bi·∫øt th√¥ng tin v·ªÅ c√¥ng ty VNM?",
            "Gi√° VCB trong 3 th√°ng g·∫ßn nh·∫•t?",
            "T√≠nh SMA 20 ng√†y cho HPG",
            "RSI c·ªßa VIC hi·ªán t·∫°i?"
        ],
        "usage": {
            "endpoint": "POST /api/chat",
            "body": {
                "question": "your question here"
            }
        }
    }


@app.on_event("startup")
async def startup_event():
    """Run on application startup"""
    logger.info("=" * 60)
    logger.info("Financial Agent API Starting...")
    logger.info("=" * 60)
    
    # Pre-initialize agent
    try:
        get_agent()
        logger.info("‚úÖ Agent initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize agent: {e}")
    
    logger.info("üöÄ API ready to accept requests")


@app.post("/api/upload", response_model=FileAnalysisResponse)
async def upload_file(
    file: UploadFile = File(...),
    question: str = Form(default="")
):
    """
    Upload v√† ph√¢n t√≠ch file (·∫£nh b√°o c√°o t√†i ch√≠nh ho·∫∑c Excel)
    
    Args:
        file: File ·∫£nh (PNG, JPG, PDF) ho·∫∑c Excel (.xlsx, .xls)
        question: C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m (optional)
        
    Returns:
        FileAnalysisResponse v·ªõi k·∫øt Qu·∫£Analysis
        
    Example:
        POST /api/upload
        Form data:
        - file: [binary file]
        - question: "Ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh n√†y"
    """
    temp_path = None
    try:
        file_extension = Path(file.filename).suffix.lower()
        
        # Ki·ªÉm tra lo·∫°i file
        image_types = ["image/png", "image/jpeg", "image/jpg", "application/pdf"]
        excel_types = [
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-excel",
            "application/excel"
        ]
        
        is_image = file.content_type in image_types or file_extension in ['.png', '.jpg', '.jpeg', '.pdf']
        is_excel = file.content_type in excel_types or file_extension in ['.xlsx', '.xls']
        
        if not (is_image or is_excel):
            raise HTTPException(
                status_code=400,
                detail=f"Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. H·ªó tr·ª£: PNG, JPG, PDF, XLSX, XLS. Nh·∫≠n ƒë∆∞·ª£c: {file.content_type}"
            )
        
        # L∆∞u file t·∫°m
        logger.info(f"Processing file: {file.filename}")
        
        with tempfile.NamedTemporaryFile(
            delete=False,
            suffix=file_extension,
            dir=tempfile.gettempdir()
        ) as temp_file:
            temp_path = temp_file.name
            content = await file.read()
            temp_file.write(content)
            logger.info(f"File saved to: {temp_path}")
        
        # X·ª≠ l√Ω theo lo·∫°i file
        if is_excel:
            logger.info("Processing Excel file...")
            result = analyze_excel_to_markdown(temp_path)
            markdown_output = result.get('markdown', '')
            analysis = markdown_output
            
            # G·ª≠i Markdown cho Gemini ph√¢n t√≠ch v·ªõi system prompt t√πy ch·ªânh
            try:
                from src.llm.llm_factory import LLMFactory
                
                logger.info("Sending Excel data to Gemini for analysis...")
                
                # ƒê·ªçc system prompt m·∫∑c ƒë·ªãnh t·ª´ file
                system_prompt_path = Path(__file__).parent.parent / "agent" / "prompts" / "excel_analysis_prompt.txt"
                if system_prompt_path.exists():
                    with open(system_prompt_path, 'r', encoding='utf-8') as f:
                        base_system_prompt = f.read()
                else:
                    base_system_prompt = "B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh chuy√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh doanh nghi·ªáp. H√£y ph√¢n t√≠ch d·ªØ li·ªáu sau m·ªôt c√°ch chi ti·∫øt, chuy√™n s√¢u v√† ƒë∆∞a ra nh·ªØng insights qu√Ω gi√° cho nh√† ƒë·∫ßu t∆∞."
                
                # K·∫øt h·ª£p system prompt m·∫∑c ƒë·ªãnh + user instruction
                if question.strip():
                    system_prompt = f"{base_system_prompt}\n\n---\n\nY√äU C·∫¶U T·ª™ NG∆Ø·ªúI D√ôNG:\n{question}"
                else:
                    system_prompt = base_system_prompt
                
                # G·ªçi Gemini tr·ª±c ti·∫øp
                llm = LLMFactory.get_llm()
                from langchain_core.messages import SystemMessage, HumanMessage
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=f"Ph√¢n t√≠ch d·ªØ li·ªáu t√†i ch√≠nh sau:\n\n{markdown_output}")
                ]
                
                response = await llm.ainvoke(messages)
                analysis = response.content
                
                logger.info("‚úì Gemini analysis completed")
                
            except Exception as e:
                logger.error(f"Error sending to Gemini: {e}")
                analysis = markdown_output
            
            return FileAnalysisResponse(
                success=result['success'],
                report_type="Excel Financial Data",
                extracted_text=markdown_output,
                analysis=analysis,
                message=result['message']
            )
        
        else:  # is_image
            logger.info("Processing image file...")
            result = analyze_financial_report(temp_path)
            analysis = result.analysis
            
            # G·ª≠i ·∫£nh analysis ƒë·∫øn Gemini v·ªõi system prompt n·∫øu c√≥ c√¢u h·ªèi
            if question.strip():
                try:
                    from src.llm.llm_factory import LLMFactory
                    
                    logger.info("Sending image analysis to Gemini for processing...")
                    
                    # ƒê·ªçc system prompt t·ª´ file
                    system_prompt_path = Path(__file__).parent.parent / "agent" / "prompts" / "excel_analysis_prompt.txt"
                    if system_prompt_path.exists():
                        with open(system_prompt_path, 'r', encoding='utf-8') as f:
                            base_system_prompt = f.read()
                    else:
                        base_system_prompt = "B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh chuy√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh doanh nghi·ªáp. H√£y ph√¢n t√≠ch d·ªØ li·ªáu sau m·ªôt c√°ch chi ti·∫øt, chuy√™n s√¢u v√† ƒë∆∞a ra nh·ªØng insights qu√Ω gi√° cho nh√† ƒë·∫ßu t∆∞."
                    
                    # K·∫øt h·ª£p system prompt + user question
                    system_prompt = f"{base_system_prompt}\n\n---\n\nY√äU C·∫¶U T·ª™ NG∆Ø·ªúI D√ôNG:\n{question}"
                    
                    # G·ªçi Gemini
                    llm = LLMFactory.get_llm()
                    from langchain_core.messages import SystemMessage, HumanMessage
                    
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=f"D·ªØ li·ªáu t·ª´ b√°o c√°o t√†i ch√≠nh (ƒë√£ OCR):\n\n{result.extracted_text}")
                    ]
                    
                    response = await llm.ainvoke(messages)
                    analysis = response.content
                    
                    logger.info("‚úì Gemini analysis completed for image")
                    
                except Exception as e:
                    logger.warning(f"Error sending image analysis to Gemini: {e}")
                    analysis = result.analysis
            
            return FileAnalysisResponse(
                success=result.success,
                report_type=result.report_type,
                extracted_text=result.extracted_text,
                analysis=analysis,
                message=result.message
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}"
        )
    finally:
        # X√≥a file t·∫°m
        if temp_path:
            try:
                if os.path.exists(temp_path):
                    # Th·ª≠ x√≥a file
                    os.remove(temp_path)
                    logger.info(f"Temp file deleted: {temp_path}")
            except PermissionError:
                # File ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng - schedule x√≥a sau
                try:
                    import atexit
                    atexit.register(lambda: os.remove(temp_path) if os.path.exists(temp_path) else None)
                    logger.debug(f"Scheduled temp file cleanup: {temp_path}")
                except Exception as cleanup_err:
                    logger.warning(f"Could not schedule cleanup: {cleanup_err}")
            except Exception as e:
                logger.warning(f"Error deleting temp file: {e}")


class ExcelAnalysisResponse(BaseModel):
    """Excel analysis response model"""
    success: bool
    file_name: str
    sheet_count: int
    sheet_names: List[str]
    markdown: str
    analysis: Optional[str] = None
    message: str


@app.post("/api/upload-excel", response_model=ExcelAnalysisResponse)
async def upload_excel_file(
    file: UploadFile = File(...),
    question: str = Form(default="")
):
    """
    Upload v√† ph√¢n t√≠ch file Excel (d·ªØ li·ªáu t√†i ch√≠nh)
    
    Args:
        file: File Excel (.xlsx, .xls)
        question: C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ th√™m (optional)
        
    Returns:
        ExcelAnalysisResponse v·ªõi Markdown v√† ph√¢n t√≠ch
        
    Example:
        POST /api/upload-excel
        Form data:
        - file: [binary excel file]
        - question: "Ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh n√†y"
    """
    temp_path = None
    try:
        # Ki·ªÉm tra lo·∫°i file
        allowed_types = [
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            "application/vnd.ms-excel",
            "application/excel"
        ]
        if file.content_type not in allowed_types:
            raise HTTPException(
                status_code=400,
                detail=f"Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. H·ªó tr·ª£: .xlsx, .xls. Nh·∫≠n ƒë∆∞·ª£c: {file.content_type}"
            )
        
        # L∆∞u file t·∫°m
        logger.info(f"Processing Excel file: {file.filename}")
        
        with tempfile.NamedTemporaryFile(
            delete=False,
            suffix=Path(file.filename).suffix,
            dir=tempfile.gettempdir()
        ) as temp_file:
            temp_path = temp_file.name
            content = await file.read()
            temp_file.write(content)
            logger.info(f"File saved to: {temp_path}")
        
        # Ph√¢n t√≠ch Excel th√†nh Markdown
        logger.info("Converting Excel to Markdown...")
        result = analyze_excel_to_markdown(temp_path)
        
        # N·∫øu c√≥ c√¢u h·ªèi, g·ª≠i Markdown cho Gemini/Agent x·ª≠ l√Ω
        analysis = ""
        if question.strip():
            try:
                agent_instance = get_agent()
                combined_question = f"{question}\n\nD·ªØ li·ªáu t√†i ch√≠nh (Markdown):\n\n{result['markdown']}"
                answer = await agent_instance.aquery(combined_question)
                analysis = answer
                logger.info("‚úì Agent analysis completed")
            except Exception as e:
                logger.warning(f"Error sending to agent: {e}")
                analysis = ""
        
        return ExcelAnalysisResponse(
            success=result['success'],
            file_name=result['file_name'],
            sheet_count=result['sheet_count'],
            sheet_names=result['sheet_names'],
            markdown=result['markdown'],
            analysis=analysis,
            message=result['message']
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing Excel file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"L·ªói x·ª≠ l√Ω file Excel: {str(e)}"
        )
    finally:
        # X√≥a file t·∫°m
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                logger.info(f"Temp file deleted: {temp_path}")
            except Exception as e:
                logger.warning(f"Error deleting temp file: {e}")


@app.on_event("shutdown")
async def shutdown_event():
    """Run on application shutdown"""
    logger.info("Shutting down Financial Agent API...")
