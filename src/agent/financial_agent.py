"""
Financial Agent - Agent ch√≠nh s·ª≠ d·ª•ng LangGraph v√† ReAct pattern
"""

import logging
import os
import asyncio
from typing import Literal
from pathlib import Path

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
import json

from .state import AgentState
from ..llm import LLMFactory
from ..tools import get_all_tools
from ..core.summarization import summarize_tool_result
from ..core.tool_config import ToolsConfig, DEFAULT_TOOLS_CONFIG

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FinancialAgent:
    """
    Financial Agent s·ª≠ d·ª•ng LangGraph v√† ReAct pattern
    
    Workflow:
    1. User input ‚Üí HumanMessage
    2. Agent node: LLM ph√¢n t√≠ch v√† quy·∫øt ƒë·ªãnh d√πng tool n√†o
    3. Tool node: Th·ª±c thi tools (get_company_info, get_historical_data, calculate_sma, calculate_rsi)
    4. Agent node: Nh·∫≠n k·∫øt qu·∫£ tool, t·ªïng h·ª£p v√† tr·∫£ l·ªùi
    5. End: Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
    """
    
    def __init__(self, config: ToolsConfig = None):
        """
        Initialize Financial Agent.
        
        Args:
            config: Optional ToolsConfig for tool and feature configuration
        """
        logger.info("Initializing Financial Agent...")
        
        # Use provided config or default
        self.config = config or DEFAULT_TOOLS_CONFIG
        logger.info(f"Using config: tools={self.config.enabled_tools}, RAG={self.config.allow_rag}")
        
        # Get LLM from factory
        self.llm = LLMFactory.get_llm()
        
        # Get all tools (config-filtered)
        self.tools = get_all_tools(self.config)
        tool_names = []
        for t in self.tools:
            if hasattr(t, 'name'):
                tool_names.append(t.name)
            elif isinstance(t, dict) and 'name' in t:
                tool_names.append(t['name'])
            else:
                tool_names.append(str(t))
        logger.info(f"Loaded {len(self.tools)} tools: {tool_names}")
        
        # Load system prompt
        prompts_dir = Path(__file__).parent / "prompts"
        with open(prompts_dir / "system_prompt.txt", "r", encoding="utf-8") as f:
            self.system_prompt = f.read().strip()
        
        # Create old-style LangGraph workflow (kept for backward compatibility)
        self.app = self._create_graph()
        
        # Import configuration for workflow version selection
        from ..core.config import settings
        
        # Load all available workflows
        # Create legacy 10-node LangGraph workflow (fallback)
        from ..core.langgraph_workflow import get_langgraph_workflow
        self.langgraph_workflow = get_langgraph_workflow(self)
        
        # Create V3 workflow (8-node)
        try:
            from ..core.langgraph_workflow_v3 import LangGraphWorkflowV3
            self.langgraph_workflow_v3 = LangGraphWorkflowV3(self)
            logger.info("‚úÖ V3 workflow loaded (8-node enhanced architecture)")
        except Exception as e:
            logger.warning(f"‚ùå V3 workflow not available: {e}")
            self.langgraph_workflow_v3 = None
        
        # Create V4 workflow (13-node - latest)
        try:
            from ..core.langgraph_workflow_v4 import LangGraphWorkflowV4
            self.langgraph_workflow_v4 = LangGraphWorkflowV4(self, enable_observer=settings.WORKFLOW_OBSERVER_ENABLED)
            logger.info("‚úÖ V4 workflow loaded (13-node complete architecture)")
        except Exception as e:
            logger.warning(f"‚ùå V4 workflow not available: {e}")
            self.langgraph_workflow_v4 = None
        
        # Determine default workflow version based on configuration
        self.workflow_version = settings.WORKFLOW_VERSION
        self.canary_rollout_percentage = settings.CANARY_ROLLOUT_PERCENTAGE
        
        logger.info(f"üìä Workflow Configuration:")
        logger.info(f"   Default version: {self.workflow_version}")
        logger.info(f"   Canary rollout: {self.canary_rollout_percentage}%")
        logger.info(f"   Observer enabled: {settings.WORKFLOW_OBSERVER_ENABLED}")
        
        logger.info("‚úÖ Financial Agent initialized successfully!")
        logger.info("   - Old-style workflow: self.app (backward compatible)")
        logger.info("   - Legacy 10-node: self.langgraph_workflow (fallback)")
        if self.langgraph_workflow_v3:
            logger.info("   - V3 (8-node): self.langgraph_workflow_v3")
        if self.langgraph_workflow_v4:
            logger.info("   - V4 (13-node): self.langgraph_workflow_v4 ‚≠ê")
    
    def _get_tool_descriptions(self) -> str:
        """Get formatted tool descriptions for prompt"""
        descriptions = []
        for tool in self.tools:
            try:
                # Handle both ToolCall and regular tools
                tool_name = getattr(tool, 'name', 'unknown')
                tool_desc = getattr(tool, 'description', 'No description')
                
                if hasattr(tool, 'args_schema') and tool.args_schema:
                    args = tool.args_schema.schema().get('properties', {}).keys()
                    args_str = ', '.join(args) if args else 'None'
                else:
                    args_str = 'None'
                
                descriptions.append(
                    f"- {tool_name}: {tool_desc}\n"
                    f"  Arguments: {args_str}"
                )
            except Exception as e:
                logger.warning(f"Error getting tool description: {e}")
                continue
        
        return "\n\n".join(descriptions)
    
    async def _agent_node(self, state: AgentState) -> AgentState:
        """
        Agent node - LLM ph√¢n t√≠ch v√† quy·∫øt ƒë·ªãnh
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with LLM response
        """
        logger.info("="*30)
        logger.info(">>> AGENT NODE INVOKED")
        logger.info("="*30)
        
        # Log the incoming message state for debugging
        message_count = len(state["messages"])
        logger.info(f"üì¨ Message count in state: {message_count}")
        
        # Context window awareness: check token estimate
        try:
            from ..utils.summarization import estimate_message_tokens, should_compress_history
            token_count = estimate_message_tokens(state["messages"])
            logger.info(f"üìä Estimated tokens: {token_count}/6000")
            
            # If approaching context limit, log warning
            if token_count > 5000:
                logger.warning(f"‚ö†Ô∏è Context window approaching limit ({token_count} tokens)")
        except Exception as e:
            logger.debug(f"Token estimation skipped: {e}")
        
        # CHECK FOR GREETING FIRST - before processing any messages
        # List of simple greetings and non-financial queries that shouldn't trigger tool calls
        greetings = [
            "hello", "hi", "hey", "xin ch√†o", "ch√†o", "ch√†o b·∫°n",
            "how are you", "b·∫°n kh·ªèe kh√¥ng", "nh∆∞ th·∫ø n√†o",
            "thanks", "c·∫£m ∆°n", "thank you", "thank",
            "help me", "gi√∫p t√¥i", "h·ªó tr·ª£",
            "ok", "okay", "ƒë∆∞·ª£c", "t·ªët"
        ]
        
        # Check if the last message is a human greeting (not a tool result)
        if state["messages"]:
            last_msg = state["messages"][-1]
            if isinstance(last_msg, HumanMessage):
                query_text = str(last_msg.content).lower().strip()
                
                # Check if query is EXACTLY a greeting or is a greeting with only punctuation
                # e.g., "hello" or "hello?" or "hello!" should match, but "hello world" should not
                query_clean = query_text.rstrip('?!.,;:')  # Remove trailing punctuation
                
                is_greeting = any(
                    query_clean == g or  # Exact match after removing punctuation
                    (query_text == g) or  # Exact match as-is
                    (query_clean.startswith(g + ' ') and len(query_clean.split()) == 2 and 
                     query_clean.split()[1] in ['?', '!', '.', ',', ';', ':'])  # Greeting + punctuation
                    for g in greetings
                )
                
                if is_greeting:
                    logger.info(f"‚úì DETECTED GREETING QUERY: '{query_text}'")
                    logger.info(f"--- Responding conversationally without tools ---")
                    conversational_response = AIMessage(
                        content="T√¥i l√† tr·ª£ l√Ω t∆∞ v·∫•n t√†i ch√≠nh. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ ch·ª©ng kho√°n Vi·ªát Nam kh√¥ng? T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi: th√¥ng tin c√¥ng ty, d·ªØ li·ªáu gi√° c·ªï phi·∫øu, ph√¢n t√≠ch k·ªπ thu·∫≠t (SMA, RSI), th√¥ng tin c·ªï ƒë√¥ng, ban l√£nh ƒë·∫°o, v.v."
                    )
                    return {"messages": state["messages"] + [conversational_response]}
        
        if state["messages"]:
            last_msg = state["messages"][-1]
            msg_type = type(last_msg).__name__
            msg_preview = str(last_msg.content)[:100] if hasattr(last_msg, 'content') else str(last_msg)[:100]
            logger.info(f"üìù Last message type: {msg_type}")
            logger.info(f"   Preview: {msg_preview}")
            
            # Count message types in state
            msg_types_count = {
                'HumanMessage': sum(1 for m in state["messages"] if isinstance(m, HumanMessage)),
                'AIMessage': sum(1 for m in state["messages"] if isinstance(m, AIMessage)),
                'ToolMessage': sum(1 for m in state["messages"] if isinstance(m, ToolMessage))
            }
            logger.info(f"üìä Message breakdown: {msg_types_count}")
            
            # If the last message is a ToolMessage (tool result), don't call tools again
            # Just process the result and generate final answer
            if isinstance(last_msg, ToolMessage):
                logger.info("üîß TOOL MESSAGE DETECTED - Processing tool result")
                logger.info(f"   Tool name: {last_msg.tool_calls[0]['name'] if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls else 'unknown'}")
                logger.info(f"   Result length: {len(str(last_msg.content))} chars")
                
                # Check if the tool message contains an error
                tool_content = str(last_msg.content).lower()
                if "error" in tool_content or last_msg.content.startswith("Error"):
                    logger.warning(f"Tool execution failed - Full error: {last_msg.content}")
                    # Don't ask LLM to process error - just return it directly
                    error_response = AIMessage(
                        content=f"Xin l·ªói, c√¥ng c·ª• g·∫∑p l·ªói: {last_msg.content}\n\nVui l√≤ng th·ª≠ l·∫°i v·ªõi c√°c tham s·ªë kh√°c ho·∫∑c ki·ªÉm tra m√£ ch·ª©ng kho√°n."
                    )
                    return {"messages": state["messages"] + [error_response]}
                
                # Check if we have RAG context - if yes, merge results
                has_rag = any("üìö Related Documents" in str(msg.content) for msg in state["messages"] if hasattr(msg, 'content'))
                
                # Summarization happens AFTER merging (if RAG present)
                # For now, just prepare tool content without summarizing
                tool_content = str(last_msg.content)
                
                if has_rag:
                    logger.info("   üìä RAG context detected + tool result - merging both sources")
                    # Extract RAG documents from messages
                    rag_docs_raw = None
                    original_question = None
                    for msg in state["messages"]:
                        if hasattr(msg, 'content') and "üìö Related Documents" in str(msg.content):
                            rag_docs_raw = str(msg.content)
                            # Extract original question (before RAG section)
                            original_question = rag_docs_raw.split("üìö Related Documents")[0].strip()
                            break
                    
                    if rag_docs_raw and original_question:
                        try:
                            # Re-retrieve RAG documents if available from state
                            rag_docs = state.get("_rag_documents", [])
                            if rag_docs:
                                merged_answer = await self._merge_rag_and_tool_results(
                                    rag_docs, 
                                    tool_content,  # Raw tool result, not summarized yet
                                    original_question
                                )
                                
                                # NOW summarize the merged answer if needed
                                if state.get("summarize_results", True) and len(merged_answer) > 500:
                                    try:
                                        from ..core.summarization import summarize_tool_result
                                        summary = summarize_tool_result({"content": merged_answer}, self.llm)
                                        if summary:
                                            logger.info(f"üìå Merged answer summarized: {summary[:80]}...")
                                            merged_answer = summary  # Replace entire answer with summary
                                    except Exception as e:
                                        logger.warning(f"Merged answer summarization skipped: {e}")
                                elif state.get("summarize_results") is False:
                                    logger.info("üìã Answer summarization disabled by user")
                                
                                final_response = AIMessage(content=merged_answer)
                                return {"messages": state["messages"] + [final_response]}
                        except Exception as e:
                            logger.warning(f"Result merging failed: {e}, will use tool result only")
                
                # No RAG context - summarize tool result directly
                logger.info("   ‚Üí Generating final answer based on tool output...")
                
                if state.get("summarize_results", True) and len(tool_content) > 500:
                    try:
                        from ..core.summarization import summarize_tool_result
                        summary = summarize_tool_result({"content": tool_content}, self.llm)
                        if summary:
                            logger.info(f"üìå Tool result summarized: {summary[:80]}...")
                            tool_content = tool_content + f"\n\nüìå **T√≥m t·∫Øt**: {summary}"
                    except Exception as e:
                        logger.warning(f"Tool result summarization skipped: {e}")
                elif state.get("summarize_results") is False:
                    logger.info("üìã Tool result summarization disabled by user")
                
                # Update last_msg with summarized content if changed
                if tool_content != str(last_msg.content):
                    last_msg = ToolMessage(
                        tool_call_id=last_msg.tool_call_id if hasattr(last_msg, 'tool_call_id') else "",
                        content=tool_content
                    )
                    state["messages"] = state["messages"][:-1] + [last_msg]
                
                
                # IMPORTANT: Use a STRICT result-only prompt to ensure LLM ONLY displays results
                # without explanations, code examples, or tool usage explanations
                result_only_prompt = """B·∫°n v·ª´a nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ t·ª´ m·ªôt c√¥ng c·ª• t√†i ch√≠nh.

M·ª§C TI√äU DUY NH·∫§T: Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ªØ li·ªáu cho ng∆∞·ªùi d√πng d∆∞·ªõi d·∫°ng b·∫£ng Markdown r√µ r√†ng v√† d·ªÖ ƒë·ªçc.

‚ö†Ô∏è TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
- KH√îNG gi·∫£i th√≠ch c·∫•u tr√∫c d·ªØ li·ªáu hay JSON
- KH√îNG n√™u t√™n c√°c tr∆∞·ªùng nh∆∞ "success", "data", "date", "open", "close", v.v.
- KH√îNG vi·∫øt code Python hay tham kh·∫£o json.loads, parsing
- KH√îNG n√≥i "D·ªØ li·ªáu bao g·ªìm...", "K·∫øt qu·∫£ tr·∫£ v·ªÅ...", "ƒê·ªÉ s·ª≠ d·ª•ng..."
- KH√îNG gi·∫£i th√≠ch c√°ch g·ªçi c√¥ng c·ª• ho·∫∑c tham s·ªë
- KH√îNG tr·∫£ l·ªùi b·∫±ng v√≠ d·ª• m√£ code

‚úÖ PH·∫¢I L√ÄM NGAY:
1. T·∫°o b·∫£ng Markdown v·ªõi d·ªØ li·ªáu TH·ª∞C T·∫æ (kh√¥ng ph·∫£i m·∫´u/v√≠ d·ª•)
2. Ti√™u ƒë·ªÅ c·ªôt: Ti·∫øng Vi·ªát d·ªÖ hi·ªÉu (v√≠ d·ª•: "Ng√†y", "Gi√°", "RSI", "Tr·∫°ng th√°i")
3. M·ªói h√†ng l√† d·ªØ li·ªáu th·ª±c
4. SAU b·∫£ng: Vi·∫øt ph√¢n t√≠ch ho·∫∑c k·∫øt lu·∫≠n ng·∫Øn n·∫øu c·∫ßn thi·∫øt

üí° V√ç D·ª§ ƒê√öNG (CH·ªà HI·ªÇN TH·ªä B·∫¢NG V√Ä PH√ÇN T√çCH):
| Ng√†y | Gi√° ƒë√≥ng c·ª≠a | RSI-14 | Tr·∫°ng th√°i |
|------|-------------|--------|-----------|
| 2024-12-20 | 45,200 | 68.5 | Qu√° mua |
| 2024-12-19 | 44,800 | 65.2 | Qua mua |
| 2024-12-18 | 44,500 | 62.1 | Trung t√≠nh |

Nh·∫≠n x√©t: VIC hi·ªán ƒëang ·ªü v√πng qu√° mua (RSI > 70), c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh trong ng·∫Øn h·∫°n.

‚ùå V√ç D·ª§ SAI (KH√îNG PH·∫¢I TR·∫¢ L·ªúI NH∆Ø D∆Ø∆†I ƒê√ÇY):
"K·∫øt qu·∫£ l√† m·ªôt object JSON ch·ª©a..."
"D·ªØ li·ªáu bao g·ªìm c√°c tr∆∞·ªùng..."
"ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, b·∫°n c·∫ßn..."
[r·ªìi m·ªõi hi·ªÉn th·ªã b·∫£ng]

H√ÄNH ƒê·ªòNG NGAY: ƒê·ªçc d·ªØ li·ªáu c√¥ng c·ª• tr·∫£ v·ªÅ, t·∫°o b·∫£ng Markdown, gi·∫£i th√≠ch k·∫øt qu·∫£. KH√îNG GI·∫¢I TH√çCH C·∫§U TR√öC D·ªÆ LI·ªÜU.
"""
                
                prompt = ChatPromptTemplate.from_messages([
                    ("system", result_only_prompt),
                    MessagesPlaceholder(variable_name="messages"),
                ])
                
                # Don't bind tools - just ask LLM to process the result
                chain = prompt | self.llm
                response = await chain.ainvoke({"messages": state["messages"]})
                content = response.content if hasattr(response, 'content') else str(response)
                
                # CRITICAL FIX: Validate that LLM didn't return an explanation instead of data
                # If it did, try to extract and format the data directly
                is_valid_response = self._validate_response_is_not_explanation(content)
                if not is_valid_response:
                    logger.error(f"‚ùå LLM returned explanation instead of formatted data. Attempting to extract and format data...")
                    # Try to extract actual data from tool message and format it
                    formatted_content = await self._extract_and_format_tool_data(last_msg.content)
                    if formatted_content:
                        logger.info(f"‚úì Successfully extracted and formatted tool data")
                        final_response = AIMessage(content=formatted_content)
                        return {"messages": state["messages"] + [final_response]}
                    else:
                        logger.warning(f"Could not extract data, returning LLM response as-is")
                        final_response = AIMessage(content=content)
                        return {"messages": state["messages"] + [final_response]}
                
                # Create AIMessage with the processed result
                final_response = AIMessage(content=content)
                return {"messages": state["messages"] + [final_response]}
        
        # Prepare prompt with tool descriptions
        tool_descriptions = self._get_tool_descriptions()
        system_text = self.system_prompt.replace("{tool_descriptions}", tool_descriptions)
        
        logger.info("ü§ñ PREPARING LLM INVOCATION")
        logger.info(f"   System prompt size: {len(system_text)} chars")
        logger.info(f"   Tools available: {len(self.tools)}")
        logger.info(f"   Tools allowed: {state.get('allow_tools', True)}")
        
        has_rag_context = any("üìö Related Documents" in str(msg.content) for msg in state["messages"] if hasattr(msg, 'content'))
        if has_rag_context:
            logger.info("   ‚úì RAG context detected in messages")
        else:
            logger.info("   ‚úó No RAG context in messages")
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_text),
            MessagesPlaceholder(variable_name="messages"),
        ])
        
        allow_tools = state.get('allow_tools', True)
        
        if allow_tools and has_rag_context:
            logger.info("   ‚ÑπÔ∏è  RAG present with tools enabled - will decide based on relevance")
            llm_with_tools = self.llm.bind_tools(self.tools)
            chain = prompt | llm_with_tools
        elif allow_tools and not has_rag_context:
            logger.info("   ‚úì No RAG - tools available")
            llm_with_tools = self.llm.bind_tools(self.tools)
            chain = prompt | llm_with_tools
        else:
            logger.info("   üõë Tools disabled - LLM will answer without tools")
            chain = prompt | self.llm
        
        try:
            logger.info("‚öôÔ∏è  INVOKING LLM...")
            # Invoke LLM
            response = await chain.ainvoke({"messages": state["messages"]})
            
            # ===== TOOL SELECTION REASONING LOG =====
            logger.info("[SELECT] Tool selection reasoning:")
            logger.info(f"  RAG results: {'YES' if has_rag_context else 'NO'}, score={rag_score:.2f if has_rag_context else 'N/A'} > threshold=0.50")
            
            if hasattr(response, 'tool_calls') and response.tool_calls:
                tool_names = [tc.get('name', 'unknown') for tc in response.tool_calls]
                logger.info(f"  Decision: Using TOOLS (LLM requested execution)")
                logger.info(f"  Tools: {', '.join(tool_names)}")
                
                # Check if we have RAG context - if yes, warn about unnecessary tool calls
                if has_rag_context:
                    logger.warning(f"‚ö†Ô∏è  POTENTIAL ISSUE: Tools called despite RAG context present!")
                    logger.warning(f"   Tools: {tool_names}")
                    logger.warning(f"   This might mean RAG context wasn't sufficient or system prompt wasn't followed")
                
                logger.info(f"\n‚úì TOOL CALLS DETECTED: {tool_names}")
                logger.info(f"   Tool count: {len(response.tool_calls)}")
                for i, tc in enumerate(response.tool_calls, 1):
                    tool_name = tc.get('name', 'unknown')
                    logger.info(f"   [{i}] {tool_name}")
            else:
                logger.info(f"  Decision: Using RAG or LLM-only")
                if has_rag_context:
                    logger.info(f"  Reason: RAG context sufficient (score={rag_score:.3f})")
                else:
                    logger.info(f"  Reason: General query, LLM-only response")
                logger.info(f"\n‚úì NO TOOL CALLS")
                logger.info("   ‚Üí LLM decided to answer directly")
            
            return {"messages": state["messages"] + [response]}
            
        except Exception as e:
            logger.error(f"Error in agent node: {e}")
            error_message = AIMessage(
                content=f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {str(e)}"
            )
            return {"messages": state["messages"] + [error_message]}
    
    def _should_continue(self, state: AgentState) -> Literal["tools", "end"]:
        """
        Quy·∫øt ƒë·ªãnh ti·∫øp t·ª•c hay k·∫øt th√∫c
        
        Args:
            state: Current agent state
            
        Returns:
            "tools" n·∫øu c·∫ßn g·ªçi tools, "end" n·∫øu k·∫øt th√∫c
        """
        last_message = state["messages"][-1] if state["messages"] else None
        
        if not last_message:
            logger.info("‚õî ROUTING DECISION: No messages ‚Üí END")
            return "end"
        
        # Check if last message has tool calls
        if isinstance(last_message, AIMessage) and hasattr(last_message, "tool_calls") and last_message.tool_calls:
            logger.info("üîÑ ROUTING DECISION: Tool calls found ‚Üí TOOLS NODE")
            logger.info(f"   Tools to execute: {[tc.get('name', 'unknown') for tc in last_message.tool_calls]}")
            return "tools"
        
        logger.info("‚úÖ ROUTING DECISION: No tool calls ‚Üí END")
        return "end"
    
    def _tools_node(self, state: AgentState) -> AgentState:
        """Custom tools node with answer-level summarization.
        
        Executes tools and summarizes results if >500 chars.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with tool results and summaries
        """
        # Use standard ToolNode to execute
        standard_tools = ToolNode(self.tools)
        result_state = standard_tools.invoke(state)
        
        # Post-process: add summaries to tool results if long
        if "messages" in result_state:
            messages = list(result_state["messages"])
            new_messages = []
            
            # Get tool names from previous AIMessage for context
            tool_names_map = {}
            for msg in messages:
                if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tc in msg.tool_calls:
                        tool_id = tc.get('id', tc.get('tool_call_id', ''))
                        tool_name = tc.get('name', 'unknown')
                        if tool_id:
                            tool_names_map[tool_id] = tool_name
            
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    result_content = msg.content
                    result_len = len(str(result_content))
                    
                    # Get tool name from mapping, fallback to message attribute or "unknown"
                    tool_name = tool_names_map.get(msg.tool_call_id, getattr(msg, 'name', 'unknown'))
                    
                    # Check if user wants summarization
                    should_summarize = state.get("summarize_results", True)
                    
                    # If tool result >500 chars and summarization enabled, add summary
                    if should_summarize and result_len > 500:
                        try:
                            summary = summarize_tool_result({
                                "data": result_content,
                                "tool": tool_name
                            }, self.llm)
                            if summary:
                                # Append summary to tool result
                                enhanced_content = f"{result_content}\n\nüìå **T√≥m t·∫Øt**: {summary}"
                                new_messages.append(ToolMessage(
                                    content=enhanced_content,
                                    tool_call_id=msg.tool_call_id,
                                    name=tool_name
                                ))
                            else:
                                new_messages.append(msg)
                        except Exception as e:
                            logger.warning(f"Tool result summarization skipped: {e}")
                            new_messages.append(msg)
                    elif should_summarize is False:
                        logger.info(f"üìã Tool result summarization disabled by user")
                        new_messages.append(msg)
                    else:
                        new_messages.append(msg)
                else:
                    new_messages.append(msg)
            
            result_state["messages"] = new_messages
        
        return result_state
    
    def _create_graph(self):
        """
        T·∫°o LangGraph workflow
        
        Returns:
            Compiled graph app
        """
        logger.info("Creating LangGraph workflow...")
        
        # Create state graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("agent", self._agent_node)
        workflow.add_node("tools", self._tools_node)
        
        # Set entry point
        workflow.set_entry_point("agent")
        
        # Add conditional edges
        workflow.add_conditional_edges(
            "agent",
            self._should_continue,
            {
                "tools": "tools",
                "end": END
            }
        )
        
        # After tools, always go back to agent
        workflow.add_edge("tools", "agent")
        
        # Compile graph
        app = workflow.compile()
        
        logger.info("LangGraph workflow created successfully!")
        return app
    
    async def aquery(self, question: str, user_id: str = None, session_id: str = None, conversation_history: list = None, uploaded_files: list = None, rag_documents: list = None, allow_tools: bool = True, use_rag: bool = True, summarize_results: bool = True) -> tuple:
        """
        Async query - Main entry point using simplified 2-node LangGraph workflow.
        
        ARCHITECTURE:
        1. API Level (this method):
           - File handling detection (new!)
           - Query rewriting (disambiguation)
           - RAG retrieval (dual search + filtering)
           - Conversation context assembly
        
        2. Workflow Level (langgraph_workflow):
           - Agent node: LLM decision (tool selection)
           - Tools node: Execute selected tools
           - Agent node: Final synthesis
        
        Args:
            question: User's question (Vietnamese)
            user_id: User ID for context
            session_id: Session ID for context
            conversation_history: Previous messages (list of dicts with 'role' and 'content')
            uploaded_files: List of file metadata dicts (name, type, path, size, extension) for workflow
            rag_documents: Pre-retrieved RAG documents (optional)
            allow_tools: Whether tools can be called
            use_rag: Whether RAG is enabled
            summarize_results: Whether to summarize tool outputs
            
        Returns:
            Tuple of (answer, thinking_steps) for display
        """
        try:
            logger.info(f"{'='*30}")
            logger.info(f"Processing question for user {user_id} in session {session_id}: {question}")
            logger.info(f"{'='*30}")
            
            # ========== PHASE 1: QUERY REWRITING (API LEVEL) ==========
            logger.info("="*30)
            logger.info("üîÑ QUERY REWRITING PHASE")
            logger.info("="*30)
            
            rewritten_question = question
            
            # Skip rewriting for greetings and simple queries
            import re
            greeting_patterns = [
                r"^\s*(hello|hi|xin ch√†o|ch√†o|how are you|thanks|thank you|c·∫£m ∆°n|goodbye|bye|t·∫°m bi·ªát|what'?s?\s+up|who are you|b·∫°n l√† ai)\s*[\.\?\!]*\s*$",
                r"^(sao th·∫ø)\s*[\.\?\!]*\s*$"
            ]
            is_greeting = any(re.match(p, question.lower().strip(), re.IGNORECASE) for p in greeting_patterns)
            
            if is_greeting:
                logger.info(f"Original query: {question}")
                logger.info("‚úì Query is a greeting - skipping rewriting")
            # Check if query is clear and rewrite if needed
            elif hasattr(self, '_is_query_clear') and not self._is_query_clear(question):
                try:
                    rewritten_question, reason = await asyncio.to_thread(
                        self.rewrite_query_with_context_sync,
                        question,
                        conversation_history or []
                    )
                    logger.info(f"Original query: {question}")
                    logger.info(f"Rewritten query: {rewritten_question}")
                    logger.info(f"Reason: {reason}")
                except Exception as e:
                    logger.warning(f"Rewrite failed: {e}, using original")
                    rewritten_question = question
            else:
                logger.info(f"Original query: {question}")
                logger.info("‚úì Query is clear and specific - no rewriting needed")
            
            # ========== PHASE 2: RAG RETRIEVAL (API LEVEL) ==========
            logger.info("üìö CONVERSATION CONTEXT")
            
            # Convert conversation_history from dict format to message objects
            if not conversation_history:
                conversation_history = []
            
            # Log conversation context
            if conversation_history:
                logger.info(f"   Added {len(conversation_history)} recent messages ({len(conversation_history)//2} exchanges)")
            
            # Retrieve RAG documents if enabled
            rag_results = []
            if use_rag:
                logger.info("üìñ RAG CONTEXT INTEGRATION")
                
                try:
                    from ..services.multi_collection_rag_service import get_rag_service
                    rag_service = get_rag_service()
                    
                    # Use pre-provided RAG documents or retrieve new ones
                    if rag_documents:
                        rag_results = rag_documents
                        logger.info(f"   Using pre-provided {len(rag_documents)} document(s)")
                    else:
                        # Perform RAG search with the rewritten query
                        rag_results = rag_service.search(
                            query=rewritten_question,
                            user_id=user_id or "default",
                            session_id=session_id or "default"
                        )
                    
                    # Filter by relevance threshold
                    filtered_results = []
                    for result in rag_results:
                        similarity = result.get('score', result.get('similarity', 0))
                        if similarity >= 0.30:  # Phase 2C fix: relevance threshold
                            filtered_results.append(result)
                    
                    logger.info(f"   RAG document similarities: {[str(round(r.get('score', r.get('similarity', 0)), 2)) for r in rag_results]}")
                    logger.info(f"   Filtered RAG results: {len(rag_results)} ‚Üí {len(filtered_results)} relevant documents (threshold: 0.30)")
                    
                    # Log full retrieved documents
                    if filtered_results:
                        logger.info("   Retrieved Documents:")
                        for i, doc in enumerate(filtered_results, 1):
                            title = doc.get('title', 'Unknown')
                            score = doc.get('score', doc.get('similarity', 0))
                            content = doc.get('content', doc.get('text', ''))[:500]
                            logger.info(f"   [{i}] {title} (relevance: {score:.1%})")
                            logger.info(f"       Content: {content}...")
                    else:
                        logger.info("‚ö†Ô∏è  NO RAG DOCUMENTS")
                        logger.info("   RAG was enabled but no documents matched relevance threshold")
                    
                    rag_results = filtered_results
                    
                except Exception as e:
                    logger.warning(f"RAG retrieval failed: {e}")
                    rag_results = []
            
            # ========== PHASE 3: BUILD MESSAGE HISTORY FOR WORKFLOW ==========
            
            # Convert conversation_history to LangChain message objects if needed
            from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
            
            # Build message history for the workflow
            workflow_messages = []
            
            if conversation_history:
                for msg in conversation_history:
                    if isinstance(msg, BaseMessage):
                        workflow_messages.append(msg)
                    elif isinstance(msg, dict):
                        role = msg.get('role', 'human')
                        content = msg.get('content', '')
                        if role == 'user' or role == 'human':
                            workflow_messages.append(HumanMessage(content=content))
                        elif role == 'assistant':
                            workflow_messages.append(AIMessage(content=content))
            
            # Add current question as HumanMessage
            workflow_messages.append(HumanMessage(content=question))
            
            # ========== PHASE 4: SELECT AND INVOKE WORKFLOW ==========
            logger.info("="*50)
            logger.info("üöÄ WORKFLOW SELECTION & INVOCATION")
            logger.info("="*50)
            logger.info(f"User prompt: {question}")
            logger.info(f"Rewritten: {rewritten_question}")
            logger.info(f"Files: {len(uploaded_files) if uploaded_files else 0}")
            logger.info(f"History messages: {len(workflow_messages)}")
            logger.info(f"RAG results: {len(rag_results)}")
            
            # Determine which workflow version to use for this user
            from ..core.config import settings
            selected_version = settings.should_use_workflow_version(user_id)
            logger.info(f"üìå Workflow Selection:")
            logger.info(f"   User: {user_id}")
            logger.info(f"   Default: {self.workflow_version}")
            logger.info(f"   Canary %: {self.canary_rollout_percentage}%")
            logger.info(f"   Selected: {selected_version} ‚≠ê")
            
            # Invoke the selected workflow
            if selected_version == "v4" and self.langgraph_workflow_v4:
                logger.info(f"\n‚û°Ô∏è  Using V4 (13-node complete architecture)")
                final_state = await self.langgraph_workflow_v4.invoke(
                    user_prompt=question,
                    uploaded_files=uploaded_files or [],
                    conversation_history=workflow_messages,
                    user_id=user_id or "default",
                    session_id=session_id or "default",
                    rag_results=rag_results,
                    tools_enabled=allow_tools
                )
            elif selected_version == "v3" and self.langgraph_workflow_v3:
                logger.info(f"\n‚û°Ô∏è  Using V3 (8-node enhanced architecture)")
                final_state = await self.langgraph_workflow_v3.invoke(
                    user_prompt=question,
                    uploaded_files=uploaded_files or [],
                    conversation_history=workflow_messages,
                    user_id=user_id or "default",
                    session_id=session_id or "default",
                    rag_results=rag_results,
                    tools_enabled=allow_tools
                )
            else:
                logger.info(f"\n‚û°Ô∏è  Fallback to legacy 10-node (simplified)")
                final_state = await self.langgraph_workflow.invoke(
                    user_prompt=question,
                    uploaded_files=uploaded_files or [],
                    conversation_history=workflow_messages,
                    user_id=user_id or "default",
                    session_id=session_id or "default",
                    rag_results=rag_results,
                    tools_enabled=allow_tools
                )
            
            # ========== PHASE 5: EXTRACT AND RETURN ANSWER ==========
            logger.info("="*30)
            logger.info("üìù EXTRACTING FINAL ANSWER")
            logger.info("="*30)
            
            answer = final_state.get("generated_answer", "")
            metadata = final_state.get("metadata", {})
            
            logger.info(f"Final message type: AIMessage")
            logger.info(f"Answer length: {len(answer)} chars")
            logger.info(f"Full Answer:")
            logger.info(f"{answer}")
            logger.info(f"\n‚úÖ ANSWER READY FOR USER {user_id}")
            logger.info(f"   Final length: {len(answer)} chars")
            
            # Build thinking steps from workflow result
            thinking_steps = [
                {
                    "step": 1,
                    "title": "üîç Query Analysis",
                    "description": "Analyzed and rewritten query for clarity"
                },
                {
                    "step": 2,
                    "title": "üìö Document Retrieval",
                    "description": f"Retrieved {len(rag_results)} relevant documents via RAG"
                },
                {
                    "step": 3,
                    "title": "ü§ñ Processing with Agent",
                    "description": "Agent analyzed context and decided on tools"
                },
                {
                    "step": 4,
                    "title": "‚úÖ Answer Ready",
                    "description": f"Generated final answer using LLM + context"
                }
            ]
            
            return answer, thinking_steps
            
        except Exception as e:
            logger.error(f"Error in aquery: {e}")
            import traceback
            traceback.print_exc()
            return f"Error processing query: {str(e)}", []
    
    async def aquery_legacy(self, question: str, user_id: str = None, session_id: str = None, conversation_history: list = None, rag_documents: list = None, allow_tools: bool = True, use_rag: bool = True, summarize_results: bool = True) -> tuple:
        """
        Legacy async query method (kept for backward compatibility).
        Uses the old-style self.app workflow.
        
        Args:
            question: C√¢u h·ªèi ti·∫øng Vi·ªát
            user_id: User ID (optional, for context)
            session_id: Session ID (optional, for context)
            conversation_history: List of previous messages (optional)
            rag_documents: List of RAG document chunks to include in context (optional)
            allow_tools: Whether to allow tool calls (default: True)
            use_rag: Whether to use RAG documents (default: True)
            summarize_results: Whether to summarize tool results (default: True)
            
        Returns:
            Tuple of (answer, thinking_steps)
        """
        try:
            logger.info(f"Processing question for user {user_id} in session {session_id}: {question}")
            thinking_steps = []
            
            query_lower = question.lower()
            force_no_tools = any(kw in query_lower for kw in ["no tool", "dont use tool", "without tool", "kh√¥ng d√πng tool", "kh√¥ng s·ª≠ d·ª•ng tool"])
            force_tools = any(kw in query_lower for kw in ["use tool", "use this tool", "d√πng tool", "s·ª≠ d·ª•ng tool"])
            force_no_rag = any(kw in query_lower for kw in ["no rag", "dont use rag", "without rag", "kh√¥ng d√πng rag"])
            
            if force_no_tools:
                allow_tools = False
                logger.info("üö´ User explicitly disabled tools")
            elif force_tools:
                allow_tools = True
                logger.info("‚úì User explicitly enabled tools")
            
            if force_no_rag:
                use_rag = False
                rag_documents = None
                logger.info("üö´ User explicitly disabled RAG")
            
            # Step 1: Rewrite query with context
            thinking_steps.append({
                "step": 1,
                "title": "üîÑ Rewriting Query",
                "description": "Analyzing conversation context..."
            })
            
            rewritten_question, rewrite_reason = await self.rewrite_query_with_context(question, conversation_history)
            if rewritten_question != question:
                thinking_steps[-1]["result"] = f"Query rewritten: '{rewritten_question[:80]}...'"
                logger.info(f"Query rewritten: {question} ‚Üí {rewritten_question}")
            else:
                thinking_steps[-1]["result"] = "No context needed - using original query"
            
            # Step 2: Filter RAG results if available and use_rag enabled
            if rag_documents and use_rag:
                thinking_steps.append({
                    "step": 2,
                    "title": "üîç Filtering Search Results",
                    "description": f"Evaluating {len(rag_documents)} retrieved documents..."
                })
                
                filtered_docs = self.filter_rag_results(rewritten_question, rag_documents)
                thinking_steps[-1]["result"] = f"Selected {len(filtered_docs)}/{len(rag_documents)} relevant documents"
                
                # Use only filtered docs
                if filtered_docs:
                    rag_documents = filtered_docs
                else:
                    thinking_steps[-1]["result"] = "No sufficiently relevant documents found, proceeding without RAG"
                    rag_documents = None
            elif rag_documents and not use_rag:
                logger.info("RAG disabled by user preference")
                rag_documents = None
            
            # Step 3: Prepare for workflow invocation
            thinking_steps.append({
                "step": 3,
                "title": "üí≠ Processing with Agent",
                "description": "Invoking LangGraph workflow..."
            })
            
            # Build message list with conversation history
            messages = []
            
            # Add previous messages from conversation history if provided (ONLY last 2-3 turns for context, not all)
            if conversation_history:
                # Keep only the last 2 exchanges (4 messages max: user, assistant, user, assistant)
                recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
                for msg in recent_history:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role == "assistant":
                        messages.append(AIMessage(content=content))
                    else:
                        messages.append(HumanMessage(content=content))
                logger.info(f"üìö CONVERSATION CONTEXT")
                logger.info(f"   Added {len(recent_history)} recent messages ({len(recent_history)//2} exchanges)")
            
            # Add current question (with RAG context if available and relevant)
            if rag_documents and len(rag_documents) > 0:
                # Format RAG documents as context
                rag_context = self._format_rag_context(rag_documents)
                enhanced_question = f"{rewritten_question}\nüìö Related Documents:\n{rag_context}"
                messages.append(HumanMessage(content=enhanced_question))
                logger.info(f"üìñ RAG CONTEXT INTEGRATION")
                logger.info(f"   Attached {len(rag_documents)} relevant document(s) to question")
                logger.info(f"   Enhanced question length: {len(enhanced_question)} chars")
                logger.info(f"   RAG section preview:")
                for doc in rag_documents[:2]:  # Show first 2 docs
                    logger.info(f"     ‚Ä¢ {doc.get('title', 'Unknown')} (relevance: {doc.get('similarity', 0):.1%})")
            else:
                messages.append(HumanMessage(content=rewritten_question))
                if rag_documents is not None:
                    logger.info(f"\n‚ö†Ô∏è  NO RAG DOCUMENTS")
                    logger.info(f"   RAG was enabled but no documents matched relevance threshold")
            
            has_rag = any("üìö Related Documents" in str(msg.content) for msg in messages if hasattr(msg, 'content'))
            
            # Step 3: Prepare initial state
            initial_state = {
                "messages": messages,
                "allow_tools": allow_tools,
                "has_rag_context": has_rag,
                "summarize_results": summarize_results,
                "_rag_documents": rag_documents if rag_documents else []
            }
            
            logger.info("="*30)
            logger.info("üöÄ INVOKING LANGGRAPH WORKFLOW")
            logger.info("="*30)
            result = await self.app.ainvoke(initial_state)
            logger.info("="*30)
            logger.info("‚úÖ LANGGRAPH WORKFLOW COMPLETED")
            logger.info("="*30)
            
            tool_calls_made = []
            for msg in result.get("messages", []):
                if isinstance(msg, AIMessage) and hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        tool_name = tool_call.get('name', 'unknown') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'unknown')
                        tool_calls_made.append(tool_name)
            
            # Update workflow step with results
            if tool_calls_made:
                unique_tools = list(set(tool_calls_made))
                thinking_steps[-1]["result"] = f"Used {len(unique_tools)} tool(s): {', '.join(unique_tools)}"
                thinking_steps[-1]["title"] = "‚öôÔ∏è  Agent Execution (with Tools)"
            else:
                thinking_steps[-1]["result"] = "Answered from knowledge without tools"
                thinking_steps[-1]["title"] = "‚öôÔ∏è  Agent Execution"
            
            # Add final answer generation step
            thinking_steps.append({
                "step": 4,
                "title": "‚úÖ Answer Ready",
                "description": "Formatted and validated response..."
            })
            
            # Get final answer
            logger.info("="*30)
            logger.info("üìù EXTRACTING FINAL ANSWER")
            logger.info("="*30)
            final_message = result["messages"][-1]
            final_msg_type = type(final_message).__name__
            logger.info(f"Final message type: {final_msg_type}")
            
            content = final_message.content if hasattr(final_message, 'content') else str(final_message)
            
            # Handle case where content is a list of content blocks
            if isinstance(content, list):
                # Extract text from content blocks
                text_parts = []
                for block in content:
                    if isinstance(block, dict) and block.get('type') == 'text':
                        text_parts.append(block.get('text', ''))
                    elif isinstance(block, str):
                        text_parts.append(block)
                answer = ''.join(text_parts)
                logger.info(f"Extracted from {len(content)} content blocks")
            else:
                answer = str(content)
            
            logger.info(f"Answer length: {len(answer)} chars")
            logger.info(f"Answer preview: {answer[:150]}...")
            
            # Clean up JSON responses - if answer looks like JSON, convert to natural text
            answer = self._clean_json_response(answer)
            
            thinking_steps[-1]["result"] = "‚úÖ Answer generated successfully"
            
            logger.info(f"\n‚úÖ ANSWER READY FOR USER {user_id}")
            logger.info(f"   Final length: {len(answer)} chars")
            return answer, thinking_steps
            
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            error_steps = [
                {"step": 1, "title": "‚ùå Error", "result": str(e)}
            ]
            return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}", error_steps
    
    def _format_rag_context(self, documents: list) -> str:
        """
        Format RAG documents for inclusion in the question
        
        Args:
            documents: List of document chunks
            
        Returns:
            Formatted context string (content-focused, without filenames that trigger tools)
        """
        context_parts = []
        for i, doc in enumerate(documents, 1):
            title = doc.get('title', 'Unknown')
            text = doc.get('text', '')
            similarity = doc.get('similarity', 0)
            # Note: Intentionally NOT including 'source' field to prevent LLM from 
            # thinking it should call analyze_excel_file or other file tools
            
            # Limit text length
            text_preview = text[:300] + "..." if len(text) > 300 else text
            
            context_parts.append(
                f"{i}. [{title}] (Relevance: {similarity:.1%})\n"
                f"   {text_preview}\n"
            )
        
        return "\n".join(context_parts)
    
    async def _merge_rag_and_tool_results(self, rag_docs: list, tool_result: str, original_question: str) -> str:
        """
        Merge RAG documents with tool execution results for comprehensive answer.
        
        Args:
            rag_docs: List of RAG documents
            tool_result: String result from tool execution
            original_question: Original user question
            
        Returns:
            Merged response combining both sources
        """
        from langchain_core.prompts import PromptTemplate
        
        rag_context = self._format_rag_context(rag_docs)
        
        merge_template = """B·∫°n v·ª´a nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t·ª´ hai ngu·ªìn:

1. NGHI√äN C·ª®U T√ÄI LI·ªÜU (t·ª´ c∆° s·ªü d·ªØ li·ªáu):
{rag_context}

2. D·ªÆ LI·ªÜU TH·ªúI GIAN TH·ª∞C (t·ª´ c√¥ng c·ª•):
{tool_result}

NHI·ªÜM V·ª§: Vi·∫øt c√¢u tr·∫£ l·ªùi c√¢n b·∫±ng k·∫øt h·ª£p c·∫£ hai ngu·ªìn:
- N√™u ph√¢n t√≠ch t·ª´ t√†i li·ªáu tr∆∞·ªõc (b·ªëi c·∫£nh, kh√°i ni·ªám, xu h∆∞·ªõng)
- R·ªìi ƒë∆∞a d·ªØ li·ªáu th·ªùi gian th·ª±c t·ª´ c√¥ng c·ª• (s·ªë li·ªáu c·ª• th·ªÉ, ch·ªâ s·ªë)
- K·∫øt lu·∫≠n: So s√°nh hay nh·∫≠n x√©t t·ª´ c·∫£ hai

C√¢u h·ªèi g·ªëc: {original_question}

Vi·∫øt c√¢u tr·∫£ l·ªùi th·ª±c t·∫ø, kh√¥ng l·∫∑p l·∫°i "d·ªØ li·ªáu bao g·ªìm..." hay "k·∫øt qu·∫£ tr·∫£ v·ªÅ..."
"""
        
        prompt = PromptTemplate(
            template=merge_template,
            input_variables=["rag_context", "tool_result", "original_question"]
        )
        
        chain = prompt | self.llm
        response = await chain.ainvoke({
            "rag_context": rag_context,
            "tool_result": tool_result,
            "original_question": original_question
        })
        
        merged_answer = response.content if hasattr(response, 'content') else str(response)
        logger.info(f"üìä MERGED RESULT: Combined {len(rag_docs)} RAG docs + tool result")
        
        return merged_answer
    
    def _clean_json_response(self, answer: str) -> str:
        """
        Detect and clean JSON-formatted responses or explanations of JSON structure.
        If answer is explaining JSON instead of showing a table, try to extract and 
        reformat the data.
        
        Args:
            answer: Raw answer from LLM
            
        Returns:
            Cleaned answer (natural language or original if not JSON)
        """
        import json
        import re
        
        answer = answer.strip()
        
        # Check if answer looks like it's explaining JSON structure or data format
        json_explanation_keywords = [
            "ƒë·ªëi t∆∞·ª£ng json",
            "c·∫•u tr√∫c json", 
            "json ch·ª©a",
            "m·∫£ng d·ªØ li·ªáu",
            "t·ª´ng ph·∫ßn t·ª≠",
            "b·∫£n ghi d·ªØ li·ªáu",
            "th·ª≠ vi·ªán json",
            "json parsing",
            "json.loads",
            "ƒë·ªÉ s·ª≠ d·ª•ng d·ªØ li·ªáu",
            "duy·ªát qua m·∫£ng",
            "d·ªØ li·ªáu n√†y bao g·ªìm",
            "d·ªØ li·ªáu bao g·ªìm",
            "c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng",
            "ƒë·ªÉ ph√¢n t√≠ch",
            "d·ªØ li·ªáu n√†y ch·ª©a",
            "d·ªØ li·ªáu n√†y c√≥",
            "m·∫£ng ch·ª©a",
            "bao g·ªìm c√°c kh√≥a",
            "kh√≥a nh∆∞",
        ]
        
        # Check if answer is explaining how to use a tool instead of showing results
        tool_explanation_keywords = [
            "s·ª≠a ƒë·ªïi m√£",
            "c√°ch s·ª≠a",
            "th√™m tham s·ªë",
            "c·∫ßn ƒë·∫£m b·∫£o",
            "ƒë·ªÉ gi·∫£i quy·∫øt",
            "def get_",
            "officers = get_officers",
            "ƒë·ªÉ g·ªçi c√¥ng c·ª•",
            "trong v√≠ d·ª• tr√™n",
            "h√£y cho t√¥i bi·∫øt ƒë·ªÉ",
            "n·∫øu b·∫°n v·∫´n g·∫∑p",
            "ch∆∞∆°ng tr√¨nh python",
            "m√£ ngu·ªìn c·ªßa ch∆∞∆°ng tr√¨nh",
            "import requests",
            "import json",
        ]
        
        answer_lower = answer.lower()
        is_json_explanation = any(keyword in answer_lower for keyword in json_explanation_keywords)
        is_tool_explanation = any(keyword in answer_lower for keyword in tool_explanation_keywords)
        
        if is_json_explanation or is_tool_explanation:
            logger.error(f"‚ùå CRITICAL: JSON/Tool explanation detected instead of results: {answer[:100]}")
            logger.error(f"‚ùå This means the LLM ignored the system prompt instructions!")
            
            if is_tool_explanation:
                logger.error(f"‚ùå Agent is explaining tool usage instead of showing results")
            
            # Try to find JSON data embedded in the explanation text
            json_patterns = [
                r'\{[^{}]*"success"[^{}]*\}',  # JSON with success field
                r'\[\s*\{[^}]*\}\s*(?:,\s*\{[^}]*\})*\s*\]',  # Array of objects
                r'\{[^{}]*\}',  # Any JSON object
            ]
            
            json_matches = []
            for pattern in json_patterns:
                json_matches = re.findall(pattern, answer)
                if json_matches:
                    break
            
            if json_matches:
                for json_str in json_matches:
                    try:
                        # Try to parse each match
                        data = json.loads(json_str)
                        logger.error(f"‚úì Found and extracted JSON data from explanation")
                        
                        # Format the extracted data as a proper response
                        formatted = self._format_tool_result(data)
                        if formatted and formatted != answer:
                            logger.error(f"‚úì Replacing explanation with formatted result")
                            return formatted
                    except json.JSONDecodeError as e:
                        logger.warning(f"Could not parse JSON: {str(e)[:100]}")
                        continue
            
            # If we couldn't extract JSON, just return the answer as-is and flag the issue
            logger.error(f"‚ö†Ô∏è Could not extract JSON from explanation - returning answer as-is")
            logger.error(f"‚ö†Ô∏è System prompt failed to prevent tool explanation. LLM returned explanation instead of results.")
            return answer
        
        # Check if answer looks like JSON (starts with { or [)
        if answer.startswith('{') or answer.startswith('['):
            try:
                parsed = json.loads(answer)
                
                # If it's a simple object like {"name": "hello"...}, extract values
                if isinstance(parsed, dict):
                    # Try to extract a meaningful message
                    for key in ['content', 'message', 'text', 'answer', 'response']:
                        if key in parsed and isinstance(parsed[key], str):
                            return parsed[key]
                    
                    # If no standard key, return a default message
                    logger.warning(f"JSON response detected and cleaned: {answer[:100]}")
                    return "T√¥i l√† tr·ª£ l√Ω t∆∞ v·∫•n t√†i ch√≠nh. B·∫°n c√≥ c√¢u h·ªèi g√¨ v·ªÅ ch·ª©ng kho√°n Vi·ªát Nam kh√¥ng?"
                
                return answer
            except json.JSONDecodeError:
                # Not valid JSON, return as-is
                return answer
        
        return answer
    
    def _format_tool_result(self, data: dict) -> str:
        """
        Format extracted tool result as a proper Markdown table response.
        This is a fallback method to convert JSON data to table format
        when the LLM ignores formatting instructions.
        
        Args:
            data: Extracted tool result data
            
        Returns:
            Formatted Markdown table or message
        """
        try:
            # Check if it's an RSI result
            if "indicator" in data and data.get("indicator", "").startswith("RSI"):
                if "detailed_data" in data and isinstance(data["detailed_data"], list):
                    logger.info(f"Formatting RSI result as table")
                    rows = ["|Ng√†y|Gi√° ƒë√≥ng c·ª≠a|RSI|Tr·∫°ng th√°i|"]
                    rows.append("|---|---|---|---|")
                    for item in data["detailed_data"][:10]:  # Limit to 10 rows
                        date = item.get("date", "")
                        close = item.get("close", "")
                        rsi = item.get("rsi_14", "")
                        status = item.get("status", "")
                        rows.append(f"|{date}|{close}|{rsi}|{status}|")
                    
                    # Add summary
                    summary = "\n"
                    if "analysis" in data:
                        analysis = data["analysis"]
                        if "status" in analysis:
                            status_text = analysis["status"]
                            if status_text == "OVERBOUGHT":
                                summary += "**Nh·∫≠n x√©t**: C·ªï phi·∫øu ƒëang ·ªü v√πng qu√° mua (RSI > 70), c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh trong ng·∫Øn h·∫°n."
                            elif status_text == "OVERSOLD":
                                summary += "**Nh·∫≠n x√©t**: C·ªï phi·∫øu ƒëang ·ªü v√πng qu√° b√°n (RSI < 30), c√≥ c∆° h·ªôi ph·ª•c h·ªìi."
                            else:
                                summary += "**Nh·∫≠n x√©t**: C·ªï phi·∫øu ƒëang ·ªü v√πng trung t√≠nh."
                    
                    return "\n".join(rows) + summary
            
            # Check if it's a SMA result
            if "indicator" in data and data.get("indicator", "").startswith("SMA"):
                if "detailed_data" in data and isinstance(data["detailed_data"], list):
                    logger.info(f"Formatting SMA result as table")
                    rows = ["|Ng√†y|Gi√° ƒë√≥ng c·ª≠a|SMA|Ch√™nh l·ªách|% Ch√™nh l·ªách|"]
                    rows.append("|---|---|---|---|---|")
                    for item in data["detailed_data"][:10]:  # Limit to 10 rows
                        date = item.get("date", "")
                        close = item.get("close", "")
                        sma_key = [k for k in item.keys() if k.startswith("sma_")]
                        sma_val = item.get(sma_key[0], "") if sma_key else ""
                        diff = item.get("difference", "")
                        diff_pct = item.get("difference_percent", "")
                        rows.append(f"|{date}|{close}|{sma_val}|{diff}|{diff_pct}|")
                    
                    # Add summary
                    summary = "\n"
                    if "analysis" in data:
                        analysis = data["analysis"]
                        trend = analysis.get("trend", "")
                        if trend:
                            summary += f"**Nh·∫≠n x√©t**: {trend}"
                    
                    return "\n".join(rows) + summary
            
            # Check if it's a historical data result (detailed_data key)
            if "detailed_data" in data and isinstance(data.get("detailed_data"), list):
                logger.info(f"Formatting historical data result as table")
                rows = ["|Ng√†y|M·ªü|Cao|Th·∫•p|ƒê√≥ng|Kh·ªëi l∆∞·ª£ng|"]
                rows.append("|---|---|---|---|---|---|")
                for item in data["detailed_data"][:10]:  # Limit to 10 rows
                    if isinstance(item, dict):
                        date = item.get("date", "")
                        open_p = item.get("open", "")
                        high = item.get("high", "")
                        low = item.get("low", "")
                        close = item.get("close", "")
                        volume = item.get("volume", "")
                        rows.append(f"|{date}|{open_p}|{high}|{low}|{close}|{volume}|")
                
                return "\n".join(rows)
            
            # Check if it's a historical data result (data key - older format)
            if "data" in data and isinstance(data.get("data"), list):
                logger.info(f"Formatting historical data result (old format) as table")
                rows = ["|Ng√†y|M·ªü|Cao|Th·∫•p|ƒê√≥ng|Kh·ªëi l∆∞·ª£ng|"]
                rows.append("|---|---|---|---|---|---|")
                for item in data["data"][:10]:  # Limit to 10 rows
                    if isinstance(item, dict):
                        date = item.get("date", "")
                        open_p = item.get("open", "")
                        high = item.get("high", "")
                        low = item.get("low", "")
                        close = item.get("close", "")
                        volume = item.get("volume", "")
                        rows.append(f"|{date}|{open_p}|{high}|{low}|{close}|{volume}|")
                
                return "\n".join(rows)
            
            # For other data structures, try generic formatting
            logger.warning(f"Could not identify specific tool result format, returning generic response")
            return f"K·∫øt qu·∫£ t·ª´ c√¥ng c·ª• ƒë∆∞·ª£c tr·∫£ v·ªÅ nh∆∞ng ƒë·ªãnh d·∫°ng kh√¥ng ƒë∆∞·ª£c nh·∫≠n di·ªán. Vui l√≤ng th·ª≠ l·∫°i."
            
        except Exception as e:
            logger.error(f"Error formatting tool result: {str(e)}")
            return f"L·ªói khi ƒë·ªãnh d·∫°ng k·∫øt qu·∫£: {str(e)}"
    
    def _validate_response_is_not_explanation(self, response: str) -> bool:
        """
        Validate that the response is actual formatted data, not an explanation of JSON structure.
        
        Args:
            response: The response from LLM
            
        Returns:
            True if response contains actual data/table, False if it's explaining structure
        """
        import re
        
        response_lower = response.lower()
        
        # Keywords that indicate explanation instead of data display
        bad_keywords = [
            "ƒë·ªëi t∆∞·ª£ng json",
            "c·∫•u tr√∫c json", 
            "json ch·ª©a",
            "m·∫£ng d·ªØ li·ªáu",
            "t·ª´ng ph·∫ßn t·ª≠",
            "b·∫£n ghi d·ªØ li·ªáu",
            "json.loads",
            "json parsing",
            "d·ªØ li·ªáu n√†y ch·ª©a",
            "d·ªØ li·ªáu bao g·ªìm",
            "c√≥ c√°c tr∆∞·ªùng",
            "c√°c kh√≥a",
            "m·∫£ng ch·ª©a",
            "ƒë√¢y l√† m·ªôt",
            "k·∫øt qu·∫£ l√† m·ªôt",
            "ƒë·ªÉ s·ª≠ d·ª•ng",
            "duy·ªát qua",
        ]
        
        # Check if response contains explanation keywords
        for keyword in bad_keywords:
            if keyword in response_lower:
                logger.error(f"‚ùå Found explanation keyword: '{keyword}'")
                return False
        
        # Check if response contains actual table markdown (valid response)
        if re.search(r'\|\s*[A-Za-z0-9_\u0080-\uffff\s]+\s*\|', response):
            logger.info(f"‚úì Response contains Markdown table - likely valid")
            return True
        
        # Check if response starts with common explanation patterns
        first_sentence = response.split('\n')[0].strip() if response else ""
        explanation_starters = [
            "d·ªØ li·ªáu n√†y",
            "k·∫øt qu·∫£ tr·∫£",
            "ƒë·ªÉ ",
            "ngo√†i ra",
            "theo nh∆∞",
        ]
        
        for starter in explanation_starters:
            if first_sentence.lower().startswith(starter):
                logger.error(f"‚ùå Response starts with explanation pattern: '{starter}'")
                return False
        
        # If response is very short and doesn't contain meaningful data, it's likely invalid
        if len(response.strip()) < 50 and not re.search(r'\d{4}-\d{2}', response):
            logger.warning(f"‚ö†Ô∏è Response is very short and contains no date patterns")
            # Could be valid for some types of responses, so don't fail here
        
        logger.info(f"‚úì Response appears to be valid formatted data")
        return True
    
    async def _extract_and_format_tool_data(self, tool_content: str) -> str:
        """
        Fallback: Extract JSON data from tool result and format it as a proper table.
        This is used when the LLM fails to format the data correctly.
        
        Args:
            tool_content: Raw tool message content (usually JSON string)
            
        Returns:
            Formatted Markdown table or empty string if extraction fails
        """
        import json
        
        logger.info("üîß Attempting to extract and format tool data directly...")
        
        try:
            # Try to parse the tool content as JSON
            if isinstance(tool_content, str):
                # First try direct JSON parse
                try:
                    data = json.loads(tool_content)
                except json.JSONDecodeError:
                    # If not valid JSON, try to extract JSON from the string
                    import re
                    json_match = re.search(r'\{.*\}', tool_content, re.DOTALL)
                    if json_match:
                        data = json.loads(json_match.group())
                    else:
                        logger.error(f"Could not find JSON in tool content")
                        return ""
            else:
                data = tool_content
            
            # Format based on data type
            formatted = self._format_tool_result(data)
            return formatted
            
        except Exception as e:
            logger.error(f"Failed to extract and format tool data: {e}")
            return ""
    
    async def rewrite_query_with_context(self, question: str, conversation_history: list = None) -> tuple:
        """
        Rewrite user query with context from conversation history.
        
        CRITICAL GUARDS:
        1. Max 1 rewrite per query (tracked by rewrite_count in state)
        2. Skip if filename found in query
        3. Only rewrite if ambiguous AND conversation context available
        4. Limit history to last 2 exchanges (4 messages) to prevent subject drift
        
        Args:
            question: Original user question
            conversation_history: List of previous messages (will limit to last 4)
            
        Returns:
            Tuple of (rewritten_query, reasoning)
        """
        logger.info("="*30)
        logger.info("üîÑ QUERY REWRITING PHASE")
        logger.info("="*30)
        logger.info(f"Original query: {question}")
        
        # GUARD 1: No history available
        if not conversation_history or len(conversation_history) == 0:
            logger.info("‚õî No conversation history - using original query")
            return question, "No conversation history"
        
        # GUARD 2: Check if query is clear and specific (no ambiguous pronouns)
        is_clear = self._is_query_clear(question)
        if is_clear:
            logger.info("‚úì Query is clear and specific - no rewriting needed")
            return question, "Query clear and specific"
        
        # GUARD 3: Limit history to last 2 exchanges (4 messages) to prevent subject drift
        recent_context = conversation_history[-4:] if conversation_history else []
        
        if not recent_context:
            logger.info("‚õî Insufficient conversation history - using original query")
            return question, "Insufficient history"
        
        logger.info(f"ü§î Ambiguous reference detected - attempting clarification...")
        logger.info(f"Using last {len(recent_context)} messages from history")
        
        # Format context safely
        context_str = ""
        for msg in recent_context:
            role = "User" if msg.get("role") == "user" else "Assistant"
            content = msg.get("content", "")[:100]
            context_str += f"{role}: {content}\n"
        
        logger.info(f"History context:\n{context_str[:200]}...")
        
        rewrite_prompt = f"""Based on the recent conversation, clarify what the ambiguous reference refers to.

Recent Conversation:
{context_str}

User's New Query: {question}

Rules:
1. Only clarify ambiguous pronouns (it, this, that, they, etc.)
2. Keep the query as close to original as possible
3. Replace ambiguous words with what they refer to
4. Respond ONLY with the clarified query (no explanation)

Clarified Query:"""
        
        try:
            # Use sync method wrapped in async
            import asyncio
            response = await asyncio.to_thread(
                self.llm.invoke,
                rewrite_prompt
            )
            rewritten = response.content if hasattr(response, 'content') else str(response)
            rewritten = rewritten.strip()
            
            logger.info(f"‚úì Query rewritten successfully:")
            logger.info(f"  Before: {question[:60]}...")
            logger.info(f"  After:  {rewritten[:60]}...")
            return rewritten, "Query clarified with context"
        except Exception as e:
            logger.warning(f"‚ùå Query rewrite failed: {e}")
            logger.info("   ‚Üí Proceeding with original query")
            return question, f"Rewrite failed: {str(e)[:50]}"
    
    async def _rewrite_query_if_needed(self, state: dict) -> dict:
        """
        Rewrite query ONCE if ambiguous, with guards:
        1. Skip if filename already present in query
        2. Skip if rewrite_count >= 1 (max 1 rewrite per query)
        3. Limit history to last 2 exchanges (4 messages)
        """
        user_prompt = state.get("user_prompt", "")
        uploaded_files = state.get("uploaded_files", [])
        conversation_history = state.get("conversation_history", [])
        rewrite_count = state.get("rewrite_count", 0)
        
        logger.info("====================")
        logger.info("üîÑ QUERY REWRITING PHASE")
        logger.info("====================")
        logger.info(f"Original query: {user_prompt[:80]}")
        
        # GUARD 1: Check if filename already in query
        has_filename_in_query = False
        if uploaded_files:
            first_file = uploaded_files[0].get("filename", "")
            if first_file and first_file in user_prompt:
                has_filename_in_query = True
                logger.info(f"‚úì Filename found in query, skipping rewrite")
        
        # GUARD 2: Check rewrite count
        if rewrite_count >= 1:
            logger.info(f"‚ö†Ô∏è  Rewrite limit reached ({rewrite_count}/1), skipping")
            return state
        
        # GUARD 3: Check if query is clear/specific
        is_clear = self._is_query_clear(user_prompt)
        if is_clear and (has_filename_in_query or not uploaded_files):
            logger.info("‚úì Query is clear and specific - no rewriting needed")
            state["rewritten_prompt"] = user_prompt
            state["rewrite_count"] = 0
            return state
        
        # GUARD 4: Only rewrite if ambiguous AND needs context
        if not has_filename_in_query and uploaded_files:
            # Limit history to last 2 exchanges (4 messages)
            recent_history = conversation_history[-4:] if conversation_history else []
            
            logger.info(f"ü§î Ambiguous reference detected - attempting clarification...")
            logger.info(f"Context from history: {len(recent_history)} recent messages")
            
            rewritten = await self._call_rewrite_agent(user_prompt, recent_history, uploaded_files)
            state["rewritten_prompt"] = rewritten
            state["rewrite_count"] = 1
            
            logger.info(f"‚úì Query rewritten successfully:")
            logger.info(f"  Before: {user_prompt[:60]}...")
            logger.info(f"  After:  {rewritten[:60]}...")
        else:
            state["rewritten_prompt"] = user_prompt
            state["rewrite_count"] = 0
        
        return state
    
    def _is_query_clear(self, query: str) -> bool:
        """
        Detect if query is clear/specific or ambiguous.
        Clear: contains specific company/metric names or clear intent
        Ambiguous: mostly pronouns/vague terms without specific subject
        
        Rules:
        - If query contains ambiguous pronouns alone ‚Üí AMBIGUOUS
        - If > 30% ambiguous terms ‚Üí AMBIGUOUS  
        - Otherwise ‚Üí CLEAR
        """
        ambiguous_terms = ["this", "that", "it", "it's", "summarize", "analyze", "explain", 
                          "what is it", "what about it", "tell me about it"]
        
        lowercase = query.lower()
        
        # Direct matches for common ambiguous patterns
        for term in ambiguous_terms:
            if term in lowercase:
                # Count total significant words
                words = [w for w in lowercase.split() if len(w) > 2]  # Ignore short words
                
                # If query is mostly just ambiguous pattern
                if len(words) <= 3 and (term in lowercase):
                    return False  # Ambiguous: mostly pronouns
        
        # Check proportion of ambiguous terms (lower threshold)
        ambiguous_count = sum(1 for term in ambiguous_terms if term in lowercase)
        words = [w for w in lowercase.split() if len(w) > 2]
        
        if len(words) > 0 and (ambiguous_count / len(words)) > 0.30:
            return False  # More than 30% ambiguous
        
        return True  # Clear and specific

    async def _call_rewrite_agent(
        self, 
        query: str, 
        history: List[dict], 
        uploaded_files: List[dict]
    ) -> str:
        """Rewrite query using LLM with limited history context."""
        first_file = uploaded_files[0].get("filename", "") if uploaded_files else ""
        
        history_text = ""
        if history:
            history_text = "\n".join([
                f"User: {h.get('content', '')[:100]}" if h.get('role') == 'user' 
                else f"Assistant: {h.get('content', '')[:100]}"
                for h in history
            ])
        
        prompt = f"""You are a query clarification agent. The user uploaded a file: {first_file}

Recent conversation:
{history_text or "(No recent conversation)"}

User's current ambiguous query: {query}

Your task: Clarify what the user is asking about regarding the uploaded file.
Return ONLY the clarified query (1-2 sentences), nothing else.

Clarified query:"""
    
        response = await self.llm.ainvoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)
